import os
import re
from datetime import datetime
import pandas as pd
from openpyxl import load_workbook

# ========= CONFIG =========
BASE_DIR = r"C:/Users/4251352/Documents/DMI_NCK"
XLSM_NAME = "Copie de bilan DIM 2025.xlsm"
MAP_CSV_NAME = "NCK.2024-2025.IPP.NAS.csv"
FIC_NAME = "2025M12.dmi.fic"
SAG_NAME = "SAG.DMI.xlsx"

# NEW: fichiers de config
CFG_EQUIV_XLSX = "config_equivalences.xlsx"
CFG_TARIFS_XLSX = "config_tarifs.xlsx"

OUT_FLAT_XLSX = "pharma_flat_avec_NDA.xlsx"
OUT_CONTROLE_XLSX = "pharma_controle_fichcomp.xlsx"
OUT_ABSENT_XLSX = "pharma_absents_dmi_fic.xlsx"
OUT_CONTROLE_GLOBAL_XLSX = "pharma_controle_global.xlsx"
OUT_FLUX_A_CORRIGER_XLSX = "pharma_flux_a_corriger.xlsx"

# --- Format fichcomp ATIH (1-based, inclusif) ---
FIC_NAS_START = 12
FIC_NAS_END = 31

FIC_DATE_START = 42
FIC_DATE_END = 49  # JJMMAAAA

FIC_LPP_START = 58
FIC_LPP_END = 72   # champ 15, code utile sur 7 caractères

FIC_QTE_START = 73
FIC_QTE_END = 82   # entier

FIC_AMT_START = 83
FIC_AMT_END = 92   # millièmes d'euros

# Colonnes finales flat demandées (12)
FINAL_RENAME = {
    "lppr": "LPPR",
    "ipp": "IPP",
    "nda": "NDA",
    "date_pose": "Date de pose",
    "nom": "Nom",
    "prenom": "Prénom",
    "date_naissance": "Date de naissance",
    "fournisseur": "Fournisseur",
    "reference_commerciale": "Référence commerciale",
    "lot": "Numéro de lot",
    "numero_serie": "Numéro de série",
    "designation": "Désignation",
}
FINAL_COLS = list(FINAL_RENAME.keys())

# ========= OUTILS =========
def clean_digits_id(x) -> str:
    s = "" if x is None else str(x).strip()
    if not s:
        return ""
    s = s.replace("\u00A0", " ").strip()
    s = re.sub(r"\.0$", "", s)
    s = s.upper()
    s = re.sub(r"^H(?=61\d+)", "", s)  # H61... -> 61...
    s = re.sub(r"\D+", "", s)
    return s

def norm(s):
    if s is None:
        return ""
    return str(s).strip()

def norm_lc(s):
    return norm(s).lower()

def excel_date_to_dt(x):
    if x is None or (isinstance(x, str) and x.strip() == ""):
        return pd.NaT
    if isinstance(x, (datetime,)):
        return pd.to_datetime(x, errors="coerce")
    if isinstance(x, pd.Timestamp):
        return x
    if isinstance(x, (int, float)):
        return pd.to_datetime("1899-12-30") + pd.to_timedelta(int(x), unit="D")

    s = str(x).strip()
    # si ISO (YYYY-MM-DD...), ne pas dayfirst
    if re.match(r"^\d{4}-\d{2}-\d{2}", s):
        return pd.to_datetime(s, errors="coerce", dayfirst=False)
    return pd.to_datetime(s, errors="coerce", dayfirst=True)

# ========= LECTURE CONFIG =========
def _read_xlsx_table(path: str, sheet=None) -> pd.DataFrame:
    df = pd.read_excel(path, sheet_name=sheet if sheet is not None else 0, dtype=str)
    df.columns = [str(c).strip() for c in df.columns]
    # supprime colonnes vides
    df = df.loc[:, [c for c in df.columns if c and not c.lower().startswith("unnamed")]]
    return df

def load_equivalences(path: str) -> dict[str, set[str]]:
    """
    Accepte au minimum 2 colonnes: LPPR_cible, LPP_equivalent
    Optionnel: Actif (O/N)
    """
    df = _read_xlsx_table(path)
    # normalise noms colonnes (tolérant)
    col_map = {c.lower(): c for c in df.columns}
    c_lppr = col_map.get("lppr_cible") or col_map.get("lppr") or col_map.get("lpp_cible")
    c_eq = col_map.get("lpp_equivalent") or col_map.get("equivalent") or col_map.get("lpp")
    if not c_lppr or not c_eq:
        raise ValueError("config_equivalences.xlsx: colonnes attendues: LPPR_cible et LPP_equivalent")

    c_actif = col_map.get("actif")  # optionnel

    out: dict[str, set[str]] = {}
    for _, r in df.iterrows():
        lppr = clean_digits_id(r.get(c_lppr))
        eq = clean_digits_id(r.get(c_eq))
        if not lppr or not eq:
            continue
        if c_actif:
            actif = str(r.get(c_actif, "")).strip().upper()
            if actif == "N":
                continue
        out.setdefault(lppr, set()).add(eq)
    return out

def load_tarifs(path: str):
    """
    Accepte au minimum: LPPR, Prix_unitaire_EUR
    Optionnel: Qte_attendue, Seuil_type, Actif
    Retourne:
      - price_eur: dict[lppr] -> float
      - qte_attendue: dict[lppr] -> int
      - seuil_type: dict[lppr] -> str ("total_NAS" par défaut)
    """
    df = pd.read_excel(path)  # laisse Excel typer la colonne prix (float)
    df.columns = [str(c).strip() for c in df.columns]
    col_map = {c.lower(): c for c in df.columns}

    c_lppr = col_map.get("lppr")
    c_price = col_map.get("prix_unitaire_eur") or col_map.get("prix_unitaire") or col_map.get("prix")
    if not c_lppr or not c_price:
        raise ValueError("config_tarifs.xlsx: colonnes attendues: LPPR et Prix_unitaire_EUR")

    c_qte = col_map.get("qte_attendue")  # optionnel
    c_seuil = col_map.get("seuil_type")  # optionnel
    c_actif = col_map.get("actif")       # optionnel

    price_eur: dict[str, float] = {}
    qte_attendue: dict[str, int] = {}
    seuil_type: dict[str, str] = {}

    for _, r in df.iterrows():
        lppr = clean_digits_id(r.get(c_lppr))
        if not lppr:
            continue

        if c_actif:
            actif = str(r.get(c_actif, "")).strip().upper()
            if actif == "N":
                continue

        # prix
        val_price = r.get(c_price)
        if pd.isna(val_price):
            continue
        try:
            price = float(val_price)
        except Exception:
            continue
        price_eur[lppr] = price

        # qte attendue
        if c_qte:
            val_q = r.get(c_qte)
            try:
                q = int(val_q) if not pd.isna(val_q) else 1
            except Exception:
                q = 1
            qte_attendue[lppr] = max(1, q)

        # seuil type
        if c_seuil:
            st = str(r.get(c_seuil, "")).strip()
            if st:
                seuil_type[lppr] = st

    return price_eur, qte_attendue, seuil_type

def read_sag_dmi(sag_path: str) -> pd.DataFrame:
    df = pd.read_excel(sag_path, dtype=str)
    df.columns = [str(c).strip() for c in df.columns]

    # helper O/N
    def to_bool_O(x) -> bool:
        s = "" if x is None else str(x).strip().upper()
        return s == "O"

    out = pd.DataFrame()
    out["service"] = df.get("Service", "")
    out["service_lib"] = df.get("Libellé du service", "")
    out["uh_code"] = df.get("Code de l'UH", "")
    out["uh_lib"] = df.get("Libellé de l'UH", "")

    out["ipp"] = df.get("NIP", "").apply(clean_digits_id)
    out["nda"] = df.get("NDA", "").apply(clean_digits_id)

    out["date_real"] = df.get("Date de réalisation", "").apply(excel_date_to_dt)
    out["date_real"] = pd.to_datetime(out["date_real"], errors="coerce", dayfirst=True).dt.normalize()

    out["lpp"] = df.get("Code LPP", "").apply(clean_digits_id).astype(str).str[:7]

    # quantite/prix
    q = df.get("Quantité", "")
    out["qte"] = pd.to_numeric(q, errors="coerce").fillna(0).astype(int)

    p = df.get("Prix", "")
    out["prix"] = pd.to_numeric(p, errors="coerce").fillna(0.0).astype(float)

    out["code_atih"] = df.get("Code ATIH", "")
    out["gratuit"] = df.get("Gratuit", "").apply(to_bool_O)
    out["echec_pose"] = df.get("Echec pose", "").apply(to_bool_O)
    out["desterilise"] = df.get("Déstérilisé", "").apply(to_bool_O)

    # statut SAG (exclusion)
    out["sag_exclu"] = out["gratuit"] | out["echec_pose"] | out["desterilise"]

    # garde uniquement lignes identifiables
    out = out[(out["ipp"] != "") | (out["nda"] != "")].copy()

    return out

# ========= (TON CODE EXISTANT EN DESSOUS, INCHANGÉ SAUF DICTS) =========

CODE_RE = re.compile(r"\b(\d{6,8})\b")

def build_lpp_candidates(lppr: str, equiv_map: dict[str, set[str]]) -> set[str]:
    lppr = str(lppr)
    cands = {lppr}
    cands |= set(equiv_map.get(lppr, set()))
    return {clean_digits_id(x)[:7] for x in cands if clean_digits_id(x)}

def get_row_values(ws, r, max_col=120):
    return [ws.cell(row=r, column=c).value for c in range(1, max_col + 1)]

def is_lppr_line(ws, r, max_col=120):
    row = get_row_values(ws, r, max_col=max_col)
    cells_txt = [norm(v) for v in row if v is not None]
    if not any("lppr" in t.lower() for t in cells_txt):
        return None
    text_line = " ".join(cells_txt)
    m = CODE_RE.search(text_line)
    if m:
        return m.group(1)
    if r + 1 <= ws.max_row:
        row2 = get_row_values(ws, r + 1, max_col=max_col)
        cells2_txt = [norm(v) for v in row2 if v is not None]
        text_line2 = " ".join(cells2_txt)
        m2 = CODE_RE.search(text_line2)
        if m2:
            return m2.group(1)
    return None

def looks_like_header(row):
    header_keywords = [
        "x_pk", "x_nip", "x_nda", "x_nom", "x_prenom", "x_tpose", "x_societe", "x_ref",
        "reference", "denomination", "lot", "date d'intervention", "nom", "prénom", "date de naissance", "ipp ou nda",
        "nom prénom", "date de naissance", "nip", "date d'intervention"
    ]
    row_lc = [norm_lc(v) for v in row]
    best = None
    hits = 0
    for i, cell in enumerate(row_lc):
        if any(k in cell for k in header_keywords):
            if best is None:
                best = i
            hits += 1
    if best is None:
        return False, None
    return hits >= 2, best

def clean_header_name(x):
    s = norm(x)
    s = s.replace("\u00A0", " ")
    s = re.sub(r"\s+", " ", s).strip()
    s = re.sub(r":\s*$", "", s)
    return s

def trim_header(header_cells):
    last = -1
    for i, v in enumerate(header_cells):
        if norm(v) != "":
            last = i
    if last == -1:
        return []
    return [clean_header_name(v) for v in header_cells[: last + 1]]

def detect_family(columns):
    cols_lc = [c.strip().lower() for c in columns]
    if sum(c.startswith("x_") for c in cols_lc) >= 5:
        return "A"
    if any("ipp ou nda" in c for c in cols_lc) or any(c == "reference" for c in cols_lc):
        return "B"
    if any(c == "nip" for c in cols_lc) and any("date d'intervention" in c for c in cols_lc):
        return "C"
    return "UNK"

def normalize_block(df_block, lppr, family):
    def normalize_colname(s):
        s = norm(s).lower()
        s = s.replace("\u00A0", " ")
        s = re.sub(r"\s+", " ", s).strip()
        s = re.sub(r":\s*$", "", s)
        return s

    col_map = {normalize_colname(c): c for c in df_block.columns}

    def find_col(*candidates, contains=False, startswith=False):
        keys = list(col_map.keys())
        for cand in candidates:
            cand_n = normalize_colname(cand)
            if cand_n in col_map:
                return col_map[cand_n]
            for k in keys:
                if contains and cand_n in k:
                    return col_map[k]
                if startswith and k.startswith(cand_n):
                    return col_map[k]
        return None

    out = pd.DataFrame(index=df_block.index)
    out["lppr"] = str(lppr)

    out["ipp"] = ""
    out["nda"] = ""
    out["date_pose"] = pd.NaT

    out["nom"] = ""
    out["prenom"] = ""
    out["date_naissance"] = pd.NaT

    out["fournisseur"] = ""
    out["reference_commerciale"] = ""

    out["lot"] = ""
    out["numero_serie"] = ""
    out["designation"] = ""

    if family == "A":
        c_ipp = find_col("x_nip", "x_ipp")
        c_nda = find_col("x_nda")
        c_date = find_col("x_tpose", "x_date_pose", contains=True)
        c_nom = find_col("x_nom")
        c_prenom = find_col("x_prenom")
        c_dn = find_col("x_dnaiss", contains=True)
        c_fourn = find_col("x_societe", contains=True)
        c_ref = find_col("x_ref", startswith=True)
        c_lot = find_col("x_lot", startswith=True)
        c_ns = find_col("x_no_serie", "x_noserie", contains=True)
        c_des = find_col("x_desig", "x_desi", contains=True)

        if c_ipp: out["ipp"] = df_block[c_ipp].astype(str).str.strip()
        if c_nda: out["nda"] = df_block[c_nda].astype(str).str.strip()
        if c_date: out["date_pose"] = df_block[c_date].apply(excel_date_to_dt)

        if c_nom: out["nom"] = df_block[c_nom].astype(str).str.strip()
        if c_prenom: out["prenom"] = df_block[c_prenom].astype(str).str.strip()
        if c_dn: out["date_naissance"] = df_block[c_dn].apply(excel_date_to_dt)

        if c_fourn: out["fournisseur"] = df_block[c_fourn].astype(str).str.strip()
        if c_ref: out["reference_commerciale"] = df_block[c_ref].astype(str).str.strip()

        if c_lot: out["lot"] = df_block[c_lot].astype(str).str.strip()
        if c_ns: out["numero_serie"] = df_block[c_ns].astype(str).str.strip()
        if c_des: out["designation"] = df_block[c_des].astype(str).str.strip()

    elif family == "B":
        c_date = find_col("date d'intervention", "date intervention", contains=True)
        c_nom = find_col("nom", startswith=True)
        c_prenom = find_col("prénom", "prenom", startswith=True)
        c_dn = find_col("date de naissance", contains=True)
        c_ref = find_col("reference", "référence", startswith=True)
        c_lot = find_col("lot", startswith=True)
        c_mix = find_col("ipp ou nda", contains=True)

        if c_date: out["date_pose"] = df_block[c_date].apply(excel_date_to_dt)
        if c_nom: out["nom"] = df_block[c_nom].astype(str).str.strip()
        if c_prenom: out["prenom"] = df_block[c_prenom].astype(str).str.strip()
        if c_dn: out["date_naissance"] = df_block[c_dn].apply(excel_date_to_dt)
        if c_ref: out["reference_commerciale"] = df_block[c_ref].astype(str).str.strip()
        if c_lot: out["lot"] = df_block[c_lot].astype(str).str.strip()

        if c_mix:
            vals = df_block[c_mix].astype(str).str.strip()
            mask_ipp = vals.str.startswith("8", na=False)
            mask_nda = vals.str.startswith("61", na=False)
            out.loc[mask_ipp, "ipp"] = vals[mask_ipp]
            out.loc[mask_nda, "nda"] = vals[mask_nda]

    elif family == "C":
        c_ipp = find_col("nip", startswith=True)
        c_date = find_col("date d'intervention") or find_col("date intervention", contains=True)
        c_dn = find_col("date de naissance", contains=True)
        c_np = find_col("nom prénom", "nom prenom", contains=True)

        if c_ipp: out["ipp"] = df_block[c_ipp].astype(str).str.strip()
        if c_date: out["date_pose"] = df_block[c_date].apply(excel_date_to_dt)
        if c_dn: out["date_naissance"] = df_block[c_dn].apply(excel_date_to_dt)
        if c_np: out["nom"] = df_block[c_np].astype(str).str.strip()

    for k in ("ipp", "nda"):
        out[k] = out[k].astype(str).str.replace(r"\.0$", "", regex=True).str.strip().replace({"nan": "", "None": ""})

    return out

def extract_blocks_from_sheet(ws, max_col=120):
    max_row = ws.max_row
    blocks = []
    r = 1
    while r <= max_row:
        lppr = is_lppr_line(ws, r, max_col=max_col)
        if lppr:
            header_row = None
            header_start = None
            header = None
            for rr in (r + 1, r + 2, r + 3):
                if rr > max_row:
                    break
                cand = get_row_values(ws, rr, max_col=max_col)
                ok, start = looks_like_header(cand)
                if ok:
                    header_row = rr
                    header_start = start
                    header = cand
                    break
            if header_row is None:
                r += 1
                continue

            header_cells = trim_header(header[header_start:])
            if not header_cells:
                r += 1
                continue

            data = []
            rr = header_row + 1
            empty_streak = 0
            width = len(header_cells)

            while rr <= max_row:
                if is_lppr_line(ws, rr, max_col=max_col):
                    break
                row_rr = get_row_values(ws, rr, max_col=max_col)
                vals = row_rr[header_start: header_start + width]
                if all(norm(v) == "" for v in vals):
                    empty_streak += 1
                    if empty_streak >= 3:
                        break
                else:
                    empty_streak = 0
                    data.append(vals)
                rr += 1

            df_block = pd.DataFrame(data, columns=header_cells)
            blocks.append({"lppr": lppr, "df": df_block})
            r = rr
        else:
            r += 1
    return blocks

def enrich_absents_with_sag(abs_export: pd.DataFrame, sag: pd.DataFrame, equiv_map: dict[str, set[str]], tol_days: int = 2) -> pd.DataFrame:
    abs2 = abs_export.copy()

    # colonnes de sortie
    abs2["SAG trouvé"] = False
    abs2["SAG exclu"] = False
    abs2["SAG motif exclu"] = ""
    abs2["SAG Service"] = ""
    abs2["SAG UH"] = ""
    abs2["SAG Date"] = pd.NaT
    abs2["SAG LPP"] = ""
    abs2["SAG Qté"] = 0
    abs2["SAG Prix"] = 0.0
    abs2["SAG Code ATIH"] = ""
    abs2["Statut final"] = ""

    # index SAG pour accélérer
    sag = sag.copy()
    sag["date_real"] = pd.to_datetime(sag["date_real"], errors="coerce").dt.normalize()

    # pre-joins: par NDA
    sag_by_nda = sag[sag["nda"].ne("")].copy()
    # par IPP
    sag_by_ipp = sag[sag["ipp"].ne("")].copy()

    def pick_best(cand: pd.DataFrame, target_date: pd.Timestamp):
        if cand.empty:
            return None
        if pd.isna(target_date):
            # si pas de date, prend la plus proche "any"
            return cand.iloc[0]
        cand = cand.copy()
        cand["delta"] = (cand["date_real"] - target_date).abs().dt.days
        cand = cand.sort_values(["delta"], ascending=True)
        return cand.iloc[0]

    for i, r in abs2.iterrows():
        lppr = str(r["LPPR"])
        nda = clean_digits_id(r.get("NDA"))
        ipp = clean_digits_id(r.get("IPP"))
        dpose = r.get("Date de pose")
        dpose = pd.to_datetime(dpose, errors="coerce", dayfirst=True)
        if isinstance(dpose, pd.Timestamp):
            dpose = dpose.normalize()

        cands_lpp = build_lpp_candidates(lppr, equiv_map)

        found_row = None

        # Match A: NDA + LPP (+ date proche si possible)
        if nda:
            cand = sag_by_nda[(sag_by_nda["nda"] == nda) & (sag_by_nda["lpp"].isin(cands_lpp))]
            if not cand.empty and pd.notna(dpose):
                cand = cand[(cand["date_real"] >= dpose - pd.Timedelta(days=tol_days)) &
                            (cand["date_real"] <= dpose + pd.Timedelta(days=tol_days))]
            found_row = pick_best(cand, dpose)

        # Fallback: IPP + date proche + LPP
        if found_row is None and ipp and pd.notna(dpose):
            cand = sag_by_ipp[(sag_by_ipp["ipp"] == ipp) & (sag_by_ipp["lpp"].isin(cands_lpp))]
            cand = cand[(cand["date_real"] >= dpose - pd.Timedelta(days=tol_days)) &
                        (cand["date_real"] <= dpose + pd.Timedelta(days=tol_days))]
            found_row = pick_best(cand, dpose)

        if found_row is None:
            abs2.at[i, "Statut final"] = "A relancer service (non saisi SAG)"
            continue

        # Remplissage infos SAG
        abs2.at[i, "SAG trouvé"] = True
        abs2.at[i, "SAG exclu"] = bool(found_row["sag_exclu"])
        abs2.at[i, "SAG Service"] = str(found_row.get("service", "")).strip()
        abs2.at[i, "SAG UH"] = str(found_row.get("uh_code", "")).strip()
        abs2.at[i, "SAG Date"] = found_row.get("date_real", pd.NaT)
        abs2.at[i, "SAG LPP"] = str(found_row.get("lpp", "")).strip()
        abs2.at[i, "SAG Qté"] = int(found_row.get("qte", 0))
        abs2.at[i, "SAG Prix"] = float(found_row.get("prix", 0.0))
        abs2.at[i, "SAG Code ATIH"] = str(found_row.get("code_atih", "")).strip()

        if bool(found_row["sag_exclu"]):
            motifs = []
            if bool(found_row["gratuit"]): motifs.append("Gratuit")
            if bool(found_row["echec_pose"]): motifs.append("Echec pose")
            if bool(found_row["desterilise"]): motifs.append("Déstérilisé")
            abs2.at[i, "SAG motif exclu"] = " / ".join(motifs) if motifs else "Exclu"
            abs2.at[i, "Statut final"] = f"Exclu (SAG: {abs2.at[i, 'SAG motif exclu']})"
        else:
            abs2.at[i, "Statut final"] = "Problème flux (SAG saisi / fichcomp absent)"

    return abs2

# ========= POST-NETTOYAGE =========
def post_clean(df_final: pd.DataFrame) -> pd.DataFrame:
    df_final["ipp"] = df_final["ipp"].apply(clean_digits_id)
    df_final["nda"] = df_final["nda"].apply(clean_digits_id)
    df_final["date_pose"] = pd.to_datetime(df_final["date_pose"], errors="coerce", dayfirst=True).dt.normalize()

    def is_titlecase_word(w: str) -> bool:
        if not w:
            return False
        w2 = re.sub(r"[^A-Za-zÀ-ÖØ-öø-ÿ'\-]", "", w)
        return bool(w2) and w2[0].isupper() and (w2[1:].islower() or w2[1:].replace("-", "").replace("'", "").islower())

    def split_nom_prenom(full: str):
        s = "" if full is None else str(full).strip()
        if not s:
            return "", ""
        parts = s.split()
        if len(parts) == 1:
            return s, ""
        idx = None
        for i, w in enumerate(parts):
            if is_titlecase_word(w):
                idx = i
                break
        if idx is None or idx == 0:
            return s, ""
        return " ".join(parts[:idx]).strip(), " ".join(parts[idx:]).strip()

    mask_3416095 = df_final["lppr"].astype(str).eq("3416095")
    mask_need_split = mask_3416095 & df_final["prenom"].astype(str).str.strip().eq("") & df_final["nom"].astype(str).str.strip().ne("")
    splits = df_final.loc[mask_need_split, "nom"].apply(split_nom_prenom)
    df_final.loc[mask_need_split, "nom"] = splits.apply(lambda x: x[0])
    df_final.loc[mask_need_split, "prenom"] = splits.apply(lambda x: x[1])

    no_date_and_no_nda = df_final["date_pose"].isna() & df_final["nda"].eq("")
    no_id = df_final["ipp"].eq("") & df_final["nda"].eq("")
    df_final = df_final.loc[~(no_date_and_no_nda | no_id)].copy()
    df_final.reset_index(drop=True, inplace=True)
    return df_final

# ========= MAPPING MOUVEMENTS =========
def read_mapping_csv(map_path: str) -> pd.DataFrame:
    try:
        m = pd.read_csv(map_path, sep=";", dtype=str, keep_default_na=False)
        if m.shape[1] == 1:
            raise ValueError("Bad sep")
    except Exception:
        m = pd.read_csv(map_path, sep=",", dtype=str, keep_default_na=False)

    m = m.rename(columns={"NIP": "nip", "NAS": "nas", "date.deb": "date_deb", "date.fin": "date_fin"})
    if "nip" not in m.columns and "NIP" in m.columns: m["nip"] = m["NIP"]
    if "nas" not in m.columns and "NAS" in m.columns: m["nas"] = m["NAS"]
    if "date_deb" not in m.columns and "date.deb" in m.columns: m["date_deb"] = m["date.deb"]
    if "date_fin" not in m.columns and "date.fin" in m.columns: m["date_fin"] = m["date.fin"]

    m["nip"] = m["nip"].apply(clean_digits_id)
    m["nas"] = m["nas"].apply(clean_digits_id)

    m["date_deb"] = pd.to_datetime(m["date_deb"].astype(str).str.strip(), errors="coerce", format="%Y-%m-%d").dt.normalize()
    m["date_fin"] = pd.to_datetime(m["date_fin"].astype(str).str.strip(), errors="coerce", format="%Y-%m-%d").dt.normalize()

    m = m[(m["nip"] != "") & (m["nas"] != "") & m["date_deb"].notna() & m["date_fin"].notna()].copy()
    return m[["nip", "nas", "date_deb", "date_fin"]]

def compute_nda_from_intervals(df: pd.DataFrame, mapping: pd.DataFrame) -> pd.Series:
    need = df[df["ipp"].ne("") & df["date_pose"].notna()].reset_index()[["index", "ipp", "date_pose"]].copy()
    if need.empty:
        return pd.Series([""] * len(df), index=df.index)

    cand = need.merge(mapping, left_on="ipp", right_on="nip", how="left")
    cand = cand[cand["nas"].notna()].copy()

    cand["in_interval"] = (cand["date_pose"] >= cand["date_deb"]) & (cand["date_pose"] <= cand["date_fin"])
    cand = cand[cand["in_interval"]].copy()
    if cand.empty:
        return pd.Series([""] * len(df), index=df.index)

    cand["stay_duration_days"] = (cand["date_fin"] - cand["date_deb"]).dt.days
    cand["gap_start_days"] = (cand["date_pose"] - cand["date_deb"]).dt.days.abs()

    cand = cand.sort_values(by=["index", "stay_duration_days", "gap_start_days", "date_deb"],
                            ascending=[True, True, True, False])

    best = cand.drop_duplicates(subset=["index"], keep="first").set_index("index")
    out = pd.Series([""] * len(df), index=df.index)
    out.loc[best.index] = best["nas"].astype(str).values
    return out

def fill_ipp_from_nda(df: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    mask = df["ipp"].eq("") & df["nda"].ne("")
    if not mask.any():
        return df
    tmp = df.loc[mask].reset_index().merge(mapping[["nip", "nas"]], left_on="nda", right_on="nas", how="left")
    tmp = tmp.drop_duplicates(subset=["index"], keep="first").set_index("index")
    df.loc[tmp.index, "ipp"] = tmp["nip"].fillna("").values
    return df

# ========= LECTURE FICHCOMP (.dmi.fic) =========
def parse_jjmmaaaa(s: str) -> pd.Timestamp:
    s = ("" if s is None else str(s)).strip()
    if len(s) != 8 or not s.isdigit():
        return pd.NaT
    return pd.to_datetime(s, format="%d%m%Y", errors="coerce")

def read_fichcomp_dmi(fic_path: str) -> pd.DataFrame:
    rows = []
    for enc in ("utf-8", "cp1252", "latin-1"):
        try:
            with open(fic_path, "r", encoding=enc, errors="ignore") as f:
                for line in f:
                    if not line.strip():
                        continue
                    nas_raw = line[FIC_NAS_START-1:FIC_NAS_END]
                    date_raw = line[FIC_DATE_START-1:FIC_DATE_END]
                    lpp_raw = line[FIC_LPP_START-1:FIC_LPP_END]
                    qte_raw = line[FIC_QTE_START-1:FIC_QTE_END]
                    amt_raw = line[FIC_AMT_START-1:FIC_AMT_END]

                    nas = clean_digits_id(nas_raw)
                    lpp = clean_digits_id(lpp_raw)[:7]
                    dpose = parse_jjmmaaaa(date_raw)

                    qte_s = clean_digits_id(qte_raw)
                    qte = int(qte_s) if qte_s.isdigit() else 0

                    amt_s = clean_digits_id(amt_raw)
                    amt_eur = (int(amt_s) / 1000.0) if amt_s.isdigit() else 0.0

                    if nas and lpp:
                        rows.append((nas, dpose, lpp, qte, amt_eur))
            break
        except Exception:
            continue

    df = pd.DataFrame(rows, columns=["nas", "date_fichcomp", "lpp", "qte", "montant_eur"])
    df["date_fichcomp"] = pd.to_datetime(df["date_fichcomp"], errors="coerce").dt.normalize()
    return df

def enrich_controle_with_sag(ctrl_export: pd.DataFrame, sag: pd.DataFrame, equiv_map: dict[str, set[str]], tol_days: int = 2) -> pd.DataFrame:
    """
    Ajoute au contrôle complet des indicateurs SAG :
      - SAG trouvé (LPP) : True si LPP cible/équiv trouvé sur NDA (± tol_days) ou IPP+date
      - SAG exclu : True si la ligne SAG trouvée est exclue (gratuit/échec/déstérilisé)
      - SAG motif exclu, SAG Service, SAG UH, SAG Date, SAG LPP, SAG Qté, SAG Prix, SAG Code ATIH
      - Statut global : OK / Flux à corriger / Exclu / Absent fichcomp (non saisi SAG ou non retrouvé)
    """

    out = ctrl_export.copy()

    # colonnes SAG
    out["SAG trouvé (LPP)"] = False
    out["SAG exclu"] = False
    out["SAG motif exclu"] = ""
    out["SAG Service"] = ""
    out["SAG UH"] = ""
    out["SAG Date"] = pd.NaT
    out["SAG LPP"] = ""
    out["SAG Qté"] = 0
    out["SAG Prix"] = 0.0
    out["SAG Code ATIH"] = ""
    out["Statut global"] = ""

    sag = sag.copy()
    sag["date_real"] = pd.to_datetime(sag["date_real"], errors="coerce").dt.normalize()

    sag_by_nda = sag[sag["nda"].ne("")].copy()
    sag_by_ipp = sag[sag["ipp"].ne("")].copy()

    def pick_best(cand: pd.DataFrame, target_date: pd.Timestamp):
        if cand.empty:
            return None
        if pd.isna(target_date):
            return cand.iloc[0]
        cand = cand.copy()
        cand["delta"] = (cand["date_real"] - target_date).abs().dt.days
        cand = cand.sort_values(["delta"], ascending=True)
        return cand.iloc[0]

    for i, r in out.iterrows():
        lppr = str(r["LPPR"])
        nda = clean_digits_id(r.get("NDA"))
        ipp = clean_digits_id(r.get("IPP"))
        dpose = pd.to_datetime(r.get("Date de pose"), errors="coerce", dayfirst=True)
        if isinstance(dpose, pd.Timestamp):
            dpose = dpose.normalize()

        cands_lpp = build_lpp_candidates(lppr, equiv_map)

        found_row = None

        # Match NDA + LPP (+ date)
        if nda:
            cand = sag_by_nda[(sag_by_nda["nda"] == nda) & (sag_by_nda["lpp"].isin(cands_lpp))]
            if not cand.empty and pd.notna(dpose):
                cand = cand[(cand["date_real"] >= dpose - pd.Timedelta(days=tol_days)) &
                            (cand["date_real"] <= dpose + pd.Timedelta(days=tol_days))]
            found_row = pick_best(cand, dpose)

        # Fallback IPP + date + LPP
        if found_row is None and ipp and pd.notna(dpose):
            cand = sag_by_ipp[(sag_by_ipp["ipp"] == ipp) & (sag_by_ipp["lpp"].isin(cands_lpp))]
            cand = cand[(cand["date_real"] >= dpose - pd.Timedelta(days=tol_days)) &
                        (cand["date_real"] <= dpose + pd.Timedelta(days=tol_days))]
            found_row = pick_best(cand, dpose)

        if found_row is not None:
            out.at[i, "SAG trouvé (LPP)"] = True
            out.at[i, "SAG exclu"] = bool(found_row["sag_exclu"])
            out.at[i, "SAG Service"] = str(found_row.get("service", "")).strip()
            out.at[i, "SAG UH"] = str(found_row.get("uh_code", "")).strip()
            out.at[i, "SAG Date"] = found_row.get("date_real", pd.NaT)
            out.at[i, "SAG LPP"] = str(found_row.get("lpp", "")).strip()
            out.at[i, "SAG Qté"] = int(found_row.get("qte", 0))
            out.at[i, "SAG Prix"] = float(found_row.get("prix", 0.0))
            out.at[i, "SAG Code ATIH"] = str(found_row.get("code_atih", "")).strip()

            if bool(found_row["sag_exclu"]):
                motifs = []
                if bool(found_row["gratuit"]): motifs.append("Gratuit")
                if bool(found_row["echec_pose"]): motifs.append("Echec pose")
                if bool(found_row["desterilise"]): motifs.append("Déstérilisé")
                out.at[i, "SAG motif exclu"] = " / ".join(motifs) if motifs else "Exclu"

        # Statut global (simple, filtrable)
        stat_fc = str(r.get("Statut", ""))

        if stat_fc.startswith("OK"):
            out.at[i, "Statut global"] = "OK"
        elif stat_fc.startswith("ABSENT"):
            if out.at[i, "SAG trouvé (LPP)"]:
                if out.at[i, "SAG exclu"]:
                    out.at[i, "Statut global"] = "Exclu (SAG)"
                else:
                    out.at[i, "Statut global"] = "Flux à corriger (SAG saisi / fichcomp absent)"
            else:
                out.at[i, "Statut global"] = "Absent fichcomp (SAG non retrouvé)"
        else:
            out.at[i, "Statut global"] = stat_fc or "Inconnu"

def export_excel_with_date_formats(df: pd.DataFrame, out_path: str, sheet_name: str = "data"):
    with pd.ExcelWriter(out_path, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name=sheet_name)
        ws = writer.sheets[sheet_name]
        headers = [cell.value for cell in ws[1]]

        # Ajout "SAG Date" pour le fichier absents_actionnables
        date_cols = {"Date de pose", "Date de naissance", "Date fichcomp", "SAG Date"}
        date_col_indexes = [i + 1 for i, h in enumerate(headers) if h in date_cols]

        for col_idx in date_col_indexes:
            for row in range(2, ws.max_row + 1):
                cell = ws.cell(row=row, column=col_idx)
                if cell.value is not None and cell.value != "":
                    cell.number_format = "DD/MM/YYYY"

def print_run_summary(nb_pui_rows: int, nb_absents: int, nb_flux: int, nb_exclus: int):
    print("=== Résumé ===")
    print(f"Lignes PUI (contrôle): {nb_pui_rows}")
    print(f"Absents fichcomp: {nb_absents}")
    print(f"Flux à corriger (SAG saisi / fichcomp absent): {nb_flux} (exclus: {nb_exclus})")

def main():
    xlsm_path = os.path.join(BASE_DIR, XLSM_NAME)
    map_path = os.path.join(BASE_DIR, MAP_CSV_NAME)
    fic_path = os.path.join(BASE_DIR, FIC_NAME)
    sag_path = os.path.join(BASE_DIR, SAG_NAME)

    cfg_equiv_path = os.path.join(BASE_DIR, CFG_EQUIV_XLSX)
    cfg_tarifs_path = os.path.join(BASE_DIR, CFG_TARIFS_XLSX)

    out_flat = os.path.join(BASE_DIR, OUT_FLAT_XLSX)
    out_ctrl = os.path.join(BASE_DIR, OUT_CONTROLE_XLSX)
    out_abs = os.path.join(BASE_DIR, OUT_ABSENT_XLSX)
    out_abs_action = os.path.join(BASE_DIR, "pharma_absents_actionnables.xlsx")

    # Vérification fichiers
    for p in (xlsm_path, map_path, fic_path, cfg_equiv_path, cfg_tarifs_path, sag_path):
        if not os.path.exists(p):
            raise FileNotFoundError(f"Fichier introuvable: {p}")

    # ===============================
    # 1️⃣ Chargement CONFIG
    # ===============================
    LPP_EQUIV = load_equivalences(cfg_equiv_path)
    LPP_PRICE_EUR, LPP_QTE_ATTENDUE, LPP_SEUIL_TYPE = load_tarifs(cfg_tarifs_path)

    print_run_summary(
        LPP_EQUIV,
        LPP_PRICE_EUR,
        LPP_QTE_ATTENDUE,
        LPP_SEUIL_TYPE
    )

    # ===============================
    # 2️⃣ Extraction PUI
    # ===============================
    wb = load_workbook(xlsm_path, data_only=True)
    all_norm = []

    for ws in wb.worksheets:
        blocks = extract_blocks_from_sheet(ws)
        for b in blocks:
            lppr = b["lppr"]
            df_block = b["df"]
            family = detect_family(df_block.columns)
            df_norm = normalize_block(df_block, lppr=lppr, family=family)
            all_norm.append(df_norm)

    if not all_norm:
        raise RuntimeError("Aucun bloc LPPR détecté dans le classeur.")

    df = pd.concat(all_norm, ignore_index=True)
    df = post_clean(df)

    # ===============================
    # 3️⃣ Fiabilisation NDA
    # ===============================
    mapping = read_mapping_csv(map_path)

    df["nda_pui"] = df["nda"].copy()
    df = fill_ipp_from_nda(df, mapping)
    df["nda_calcule"] = compute_nda_from_intervals(df, mapping)
    df["nda"] = df["nda_calcule"].where(df["nda_calcule"].ne(""), df["nda_pui"])
    df["nda"] = df["nda"].apply(clean_digits_id)

    nb_no_ipp = int((df["ipp"] == "").sum())
    if nb_no_ipp:
        print(f"Lignes exclues car IPP introuvable : {nb_no_ipp}")

    df = df[df["ipp"] != ""].copy()
    df.reset_index(drop=True, inplace=True)

    # ===============================
    # 4️⃣ Export flat
    # ===============================
    df_flat = df[FINAL_COLS].copy().rename(columns=FINAL_RENAME)
    export_excel_with_date_formats(df_flat, out_flat, sheet_name="flat")

    # ===============================
    # 5️⃣ Lecture FICHCOMP
    # ===============================
    fc = read_fichcomp_dmi(fic_path)

    agg_lpp = fc.groupby(["nas", "lpp"], as_index=False).agg(
        qte_lpp=("qte", "sum"),
        montant_lpp=("montant_eur", "sum"),
    )

    agg_total = fc.groupby("nas", as_index=False).agg(
        montant_total=("montant_eur", "sum"),
    )

    # ===============================
    # 6️⃣ Contrôle FICHCOMP
    # ===============================
    ctrl = df.copy()
    ctrl["lpp_cible"] = ctrl["lppr"].astype(str)

    ctrl = ctrl.merge(
        agg_lpp.rename(columns={
            "nas": "nda",
            "lpp": "lpp_cible",
            "qte_lpp": "qte_cible",
            "montant_lpp": "montant_cible"
        }),
        on=["nda", "lpp_cible"],
        how="left",
    )

    ctrl["qte_cible"] = ctrl["qte_cible"].fillna(0).astype(int)
    ctrl["montant_cible"] = ctrl["montant_cible"].fillna(0.0)

    ctrl["qte_equiv"] = 0
    ctrl["montant_equiv"] = 0.0

    for lppr, eqs in LPP_EQUIV.items():
        if not eqs:
            continue
        mask = ctrl["lppr"].astype(str).eq(lppr) & ctrl["nda"].ne("")
        if not mask.any():
            continue

        tmp = ctrl.loc[mask, ["nda"]].copy()
        tmp["key"] = 1
        eq_df = pd.DataFrame({"lpp": list(eqs), "key": 1})
        tmp = tmp.merge(eq_df, on="key").drop(columns=["key"])
        tmp = tmp.merge(agg_lpp, left_on=["nda", "lpp"], right_on=["nas", "lpp"], how="left")

        tmp["qte_lpp"] = tmp["qte_lpp"].fillna(0)
        tmp["montant_lpp"] = tmp["montant_lpp"].fillna(0.0)

        summed = tmp.groupby("nda", as_index=True).agg(
            qte_equiv=("qte_lpp", "sum"),
            montant_equiv=("montant_lpp", "sum")
        )

        ctrl.loc[mask, "qte_equiv"] = ctrl.loc[mask, "nda"].map(summed["qte_equiv"]).fillna(0).astype(int).values
        ctrl.loc[mask, "montant_equiv"] = ctrl.loc[mask, "nda"].map(summed["montant_equiv"]).fillna(0.0).values

    ctrl = ctrl.merge(
        agg_total.rename(columns={"nas": "nda"}),
        on="nda",
        how="left"
    )

    ctrl["montant_total"] = ctrl["montant_total"].fillna(0.0)
    ctrl["qte_attendue"] = ctrl["lppr"].astype(str).map(LPP_QTE_ATTENDUE).fillna(1).astype(int)

    def statut_row(r):
        lppr = str(r["lppr"])
        if r["nda"] == "":
            return "NDA manquant"
        if r["qte_cible"] >= r["qte_attendue"]:
            return "OK (LPP ciblé présent)"
        if r["qte_equiv"] > 0:
            return "OK (LPP équivalent)"

        price = LPP_PRICE_EUR.get(lppr)
        if price is not None:
            seuil = float(price) * int(r["qte_attendue"])
            st = LPP_SEUIL_TYPE.get(lppr, "total_NAS")
            amt = r["montant_total"] if st == "total_NAS" else r["montant_cible"]
            if amt < seuil:
                return "ABSENT (montant trop faible)"

        return "ABSENT (LPP non trouvé)"

    ctrl["Statut"] = ctrl.apply(statut_row, axis=1)

    ctrl_export = ctrl[FINAL_COLS].copy().rename(columns=FINAL_RENAME)
    ctrl_export["Statut"] = ctrl["Statut"].values

    ctrl_export = ctrl_export[
        ctrl_export["Date de pose"].notna() &
        (ctrl_export["Date de pose"].dt.year == 2025)
    ].copy()

    export_excel_with_date_formats(ctrl_export, out_ctrl, sheet_name="controle")

    # ===============================
    # 7️⃣ SAG (enrichissement du contrôle complet)
    # ===============================
    sag = read_sag_dmi(sag_path)

    ctrl_global = enrich_controle_with_sag(
        ctrl_export,
        sag=sag,
        equiv_map=LPP_EQUIV,
        tol_days=2
    )

    out_ctrl_global = os.path.join(BASE_DIR, OUT_CONTROLE_GLOBAL_XLSX)
    export_excel_with_date_formats(ctrl_global, out_ctrl_global, sheet_name="controle_global")

    # ===============================
    # 8️⃣ Exports ciblés (actionnables)
    # ===============================
    abs_export = ctrl_export[ctrl_export["Statut"].str.startswith("ABSENT")].copy()
    export_excel_with_date_formats(abs_export, out_abs, sheet_name="absents")

    flux_cases = ctrl_global[ctrl_global["Statut global"].str.startswith("Flux à corriger")].copy()
    out_flux = os.path.join(BASE_DIR, OUT_FLUX_A_CORRIGER_XLSX)
    export_excel_with_date_formats(flux_cases, out_flux, sheet_name="flux")

    # ===============================
    # Résumé minimal
    # ===============================
    nb_pui_rows = len(ctrl_export)
    nb_absents = int(ctrl_export["Statut"].str.startswith("ABSENT").sum())
    nb_flux = len(flux_cases)
    nb_exclus = int(ctrl_global["Statut global"].eq("Exclu (SAG)").sum())

    print_run_summary(nb_pui_rows, nb_absents, nb_flux, nb_exclus)
    print("Contrôle global:", out_ctrl_global)
    print("Flux à corriger:", out_flux)

    return out

if __name__ == "__main__":
    main()
