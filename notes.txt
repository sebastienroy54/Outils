from io import StringIO
from pathlib import Path
from typing import Dict, List, Tuple
from datetime import date, timedelta
import pandas as pd


def first_day_of_month(d: date) -> date:
    return d.replace(day=1)


def add_months(d: date, months: int) -> date:
    y = d.year + (d.month - 1 + months) // 12
    m = (d.month - 1 + months) % 12 + 1
    day = min(d.day, [31,
                      29 if (y % 4 == 0 and (y % 100 != 0 or y % 400 == 0)) else 28,
                      31, 30, 31, 30, 31, 31, 30, 31, 30, 31][m - 1])
    return date(y, m, day)


def last_day_of_month(d: date) -> date:
    return add_months(first_day_of_month(d), 1) - timedelta(days=1)


def compute_window_m2_to_end_m1(today: date) -> Tuple[date, date]:
    m1 = add_months(today, -1)
    m2 = add_months(today, -2)
    start = first_day_of_month(m2)
    end = last_day_of_month(m1)
    return start, end


def safe_read_oracle_spool_csv(csv_path: Path) -> pd.DataFrame:
    raw = csv_path.read_text(encoding="latin-1", errors="ignore").splitlines()
    kept = [ln for ln in raw if ln.count(";") >= 6]  # 7 champs attendus
    if not kept:
        return pd.DataFrame(columns=["IPP", "DATE_NAISSANCE", "NDA", "UMA", "GHM", "DATE_ENTREE", "DATE_SORTIE"])

    buf = StringIO("\n".join(kept))
    df = pd.read_csv(buf, sep=";", header=0, dtype=str, engine="python")
    return df


def analyze_csv_and_build_relations(csv_path: Path) -> pd.DataFrame:
    df = safe_read_oracle_spool_csv(csv_path)

    df = df.dropna(how="all")
    df.columns = [col.strip().upper() for col in df.columns]

    expected_cols = ["IPP", "DATE_NAISSANCE", "NDA", "UMA", "GHM", "DATE_ENTREE", "DATE_SORTIE"]
    if df.shape[1] >= len(expected_cols):
        df.columns = expected_cols + [f"COL_EXTRA_{i}" for i in range(df.shape[1] - len(expected_cols))]

    df["IPP"] = df["IPP"].astype(str).str.strip()
    df["UMA"] = df["UMA"].fillna("").astype(str).str.strip()
    df["NDA"] = df["NDA"].astype(str).str.strip()
    df["GHM"] = df["GHM"].fillna("").astype(str).str.strip()

    df["DATE_ENTREE"] = pd.to_datetime(df["DATE_ENTREE"], dayfirst=True, errors="coerce")
    df["DATE_SORTIE"] = pd.to_datetime(df["DATE_SORTIE"], dayfirst=True, errors="coerce")

    # exclusions
    df = df[~df["UMA"].isin(["540", "543"])]

    df = df.dropna(subset=["DATE_ENTREE", "DATE_SORTIE", "UMA", "IPP"])
    df = df.sort_values(["IPP", "DATE_ENTREE", "DATE_SORTIE"]).reset_index(drop=True)

    df_base = df.drop_duplicates(
        subset=["IPP", "NDA", "UMA", "DATE_ENTREE", "DATE_SORTIE"],
        keep="first"
    ).copy()

    df_contigus = df.copy()

    rows: List[Dict] = []

    def stay_key(ipp, nda, uma, de, ds):
        return (ipp, nda, uma, de, ds)

    stay_to_ghm = {}
    stay_to_ghm_full = {}
    for _, r in df_contigus.iterrows():
        ipp = r["IPP"]
        nda = r["NDA"]
        uma = r["UMA"]
        de = r["DATE_ENTREE"]
        ds = r["DATE_SORTIE"]
        ghm = str(r["GHM"]).strip()

        k = stay_key(ipp, nda, uma, de, ds)
        if ghm and ghm.upper() not in ("NAN", "NONE") and len(ghm) >= 5:
            root = ghm[:5]
            stay_to_ghm.setdefault(k, set()).add(root)
            stay_to_ghm_full.setdefault(k, {})
            stay_to_ghm_full[k].setdefault(root, ghm)

    assigned = set()

    def pair_id(k1, k2):
        return tuple(sorted([k1, k2]))

    def get_dn(g, i):
        try:
            return g.loc[i, "DATE_NAISSANCE"]
        except Exception:
            return ""

    # VRAI_DOUBLON + DOUBLON_0_NUIT
    for ipp, group in df_base.groupby("IPP", sort=False):
        g = group.reset_index(drop=True)
        n = len(g)

        for i in range(n):
            nda1, uma1, de1, ds1 = g.loc[i, ["NDA", "UMA", "DATE_ENTREE", "DATE_SORTIE"]]
            k1 = stay_key(ipp, nda1, uma1, de1, ds1)

            j = i + 1
            while j < n:
                de2 = g.loc[j, "DATE_ENTREE"]
                if pd.isna(de2):
                    j += 1
                    continue
                if de2 > ds1 and de2 != ds1:
                    break

                nda2, uma2, ds2 = g.loc[j, ["NDA", "UMA", "DATE_SORTIE"]]
                k2 = stay_key(ipp, nda2, uma2, de2, ds2)

                if nda1 == nda2:
                    j += 1
                    continue

                pid = pair_id(k1, k2)
                if pid in assigned:
                    j += 1
                    continue

                rel = None
                if (uma1 == uma2) and (de1 == de2) and (ds1 == ds2):
                    rel = "VRAI_DOUBLON"
                elif (uma1 != uma2) and (de1 == de2) and (ds1 == ds2):
                    rel = "DOUBLON_0_NUIT"
                elif (uma1 != uma2) and (de1 < ds2) and (ds1 > de2):
                    rel = "CHEVAUCHEMENT_CANDIDAT"

                if rel and rel != "CHEVAUCHEMENT_CANDIDAT":
                    assigned.add(pid)
                    rows.append({
                        "IPP": ipp,
                        "DATE_NAISSANCE": get_dn(g, i),
                        "NDA1": nda1, "UMA1": uma1, "GHM1": "",
                        "DATEE1": de1, "DATES1": ds1,
                        "NDA2": nda2, "UMA2": uma2, "GHM2": "",
                        "DATEE2": de2, "DATES2": ds2,
                        "TYPE_RELATION": rel
                    })

                j += 1

    # CONTIGUS (UMA différentes + dates collées + même racine GHM)
    for ipp, group in df_base.groupby("IPP", sort=False):
        g = group.reset_index(drop=True)
        n = len(g)

        for i in range(n):
            nda1, uma1, de1, ds1 = g.loc[i, ["NDA", "UMA", "DATE_ENTREE", "DATE_SORTIE"]]
            k1 = stay_key(ipp, nda1, uma1, de1, ds1)

            j = i + 1
            while j < n:
                de2 = g.loc[j, "DATE_ENTREE"]
                if pd.isna(de2):
                    j += 1
                    continue
                if de2 > ds1 and de2 != ds1:
                    break

                nda2, uma2, ds2 = g.loc[j, ["NDA", "UMA", "DATE_SORTIE"]]
                k2 = stay_key(ipp, nda2, uma2, de2, ds2)

                if nda1 == nda2:
                    j += 1
                    continue

                pid = pair_id(k1, k2)
                if pid in assigned:
                    j += 1
                    continue

                is_contig = (ds1 == de2) or (ds2 == de1)
                if not is_contig or uma1 == uma2:
                    j += 1
                    continue

                roots1 = stay_to_ghm.get(k1, set())
                roots2 = stay_to_ghm.get(k2, set())
                common = roots1.intersection(roots2)

                if common:
                    root = sorted(list(common))[0]
                    ghm1 = stay_to_ghm_full.get(k1, {}).get(root, "")
                    ghm2 = stay_to_ghm_full.get(k2, {}).get(root, "")

                    assigned.add(pid)
                    rows.append({
                        "IPP": ipp,
                        "DATE_NAISSANCE": get_dn(g, i),
                        "NDA1": nda1, "UMA1": uma1, "GHM1": ghm1,
                        "DATEE1": de1, "DATES1": ds1,
                        "NDA2": nda2, "UMA2": uma2, "GHM2": ghm2,
                        "DATEE2": de2, "DATES2": ds2,
                        "TYPE_RELATION": "CONTIGUS"
                    })

                j += 1

    # CHEVAUCHEMENT (après exclusions)
    for ipp, group in df_base.groupby("IPP", sort=False):
        g = group.reset_index(drop=True)
        n = len(g)

        for i in range(n):
            nda1, uma1, de1, ds1 = g.loc[i, ["NDA", "UMA", "DATE_ENTREE", "DATE_SORTIE"]]
            k1 = stay_key(ipp, nda1, uma1, de1, ds1)

            j = i + 1
            while j < n:
                de2 = g.loc[j, "DATE_ENTREE"]
                if pd.isna(de2):
                    j += 1
                    continue
                if de2 > ds1 and de2 != ds1:
                    break

                nda2, uma2, ds2 = g.loc[j, ["NDA", "UMA", "DATE_SORTIE"]]
                k2 = stay_key(ipp, nda2, uma2, de2, ds2)

                if nda1 == nda2:
                    j += 1
                    continue

                pid = pair_id(k1, k2)
                if pid in assigned:
                    j += 1
                    continue

                if (uma1 != uma2) and (de1 < ds2) and (ds1 > de2):
                    assigned.add(pid)
                    rows.append({
                        "IPP": ipp,
                        "DATE_NAISSANCE": get_dn(g, i),
                        "NDA1": nda1, "UMA1": uma1, "GHM1": "",
                        "DATEE1": de1, "DATES1": ds1,
                        "NDA2": nda2, "UMA2": uma2, "GHM2": "",
                        "DATEE2": de2, "DATES2": ds2,
                        "TYPE_RELATION": "CHEVAUCHEMENT"
                    })

                j += 1

    expected_rel_cols = [
        "IPP", "DATE_NAISSANCE",
        "NDA1", "UMA1", "GHM1", "DATEE1", "DATES1",
        "NDA2", "UMA2", "GHM2", "DATEE2", "DATES2",
        "TYPE_RELATION"
    ]
    res = pd.DataFrame(rows)
    if res.empty:
        res = pd.DataFrame(columns=expected_rel_cols)
    for c in expected_rel_cols:
        if c not in res.columns:
            res[c] = pd.Series(dtype="object")

    res = res.drop_duplicates().reset_index(drop=True)
    res["PAIR_ID"] = res.index + 1
    return res


def build_rows_instead_of_columns(df_rel: pd.DataFrame, keep_ghm: bool, site_label: str) -> pd.DataFrame:
    left_cols = {
        "IPP": "IPP",
        "DATE_NAISSANCE": "DATE_NAISSANCE",
        "NDA": "NDA1",
        "UMA": "UMA1",
        "DATE_ENTREE": "DATEE1",
        "DATE_SORTIE": "DATES1",
        "PAIR_ID": "PAIR_ID",
    }
    if keep_ghm:
        left_cols["GHM"] = "GHM1"

    df1 = df_rel[list(left_cols.values())].copy()
    df1.columns = list(left_cols.keys())
    df1["LIGNE"] = 1

    right_cols = {
        "IPP": "IPP",
        "DATE_NAISSANCE": "DATE_NAISSANCE",
        "NDA": "NDA2",
        "UMA": "UMA2",
        "DATE_ENTREE": "DATEE2",
        "DATE_SORTIE": "DATES2",
        "PAIR_ID": "PAIR_ID",
    }
    if keep_ghm:
        right_cols["GHM"] = "GHM2"

    df2 = df_rel[list(right_cols.values())].copy()
    df2.columns = list(right_cols.keys())
    df2["LIGNE"] = 2

    out = pd.concat([df1, df2], ignore_index=True)

    for c in ["DATE_ENTREE", "DATE_SORTIE"]:
        out[c] = pd.to_datetime(out[c], errors="coerce").dt.strftime("%d/%m/%Y")

    out["SITE"] = site_label
    out["COMMENTAIRE"] = ""

    if keep_ghm:
        out = out[["IPP", "DATE_NAISSANCE", "NDA", "UMA", "GHM",
                   "DATE_ENTREE", "DATE_SORTIE", "SITE", "COMMENTAIRE"]]
    else:
        out = out[["IPP", "DATE_NAISSANCE", "NDA", "UMA",
                   "DATE_ENTREE", "DATE_SORTIE", "SITE", "COMMENTAIRE"]]

    out = out.sort_values(["IPP"]).reset_index(drop=True)
    return out

import json
from pathlib import Path
from datetime import datetime, date
import pandas as pd

from oracle_runner import run_sqlplus
from mailer import send_mail
from doublons_logic import (
    compute_window_m2_to_end_m1,
    analyze_csv_and_build_relations,
    build_rows_instead_of_columns,
)

# ========= A REMPLIR =========
ORACLE_USER = "A_REMPLIR"
ORACLE_PASSWORD = "A_REMPLIR"
# =============================

BASE_DIR = Path(__file__).resolve().parent
SQL_FILE = BASE_DIR / "sql" / "contigus.sql"   # /app/sql/contigus.sql (monté)
OUTPUT_DIR = BASE_DIR / "output"               # /app/output

TNS_MAP = {
    "CCH": "//o-simpa-b1.cch.aphp.fr:10805/SIP1CCH",
    "BRC": "//o-simpa-b1.brc.aphp.fr:8105/SIP1BRC",
    "HTD": "//o-simpa-b1.htd.aphp.fr:8855/SIP1HTD"
}

MAIL_CONFIG_PATH = Path("/config/mail_doublons.json")


def main():
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    start, end = compute_window_m2_to_end_m1(date.today())
    start_str = start.strftime("%d/%m/%Y")
    end_str = end.strftime("%d/%m/%Y")

    print(f"[INFO] Période: {start_str} -> {end_str}")

    results_by_type = {k: [] for k in ["VRAI_DOUBLON", "DOUBLON_0_NUIT", "CONTIGUS", "CHEVAUCHEMENT"]}

    for site_label, dsn in TNS_MAP.items():
        print(f"[INFO] Site {site_label}: SQL*Plus ...")

        csv_path = OUTPUT_DIR / f"resultat_{site_label}.csv"
        if csv_path.exists():
            try:
                csv_path.unlink()
            except Exception:
                pass

        # contigus.sql attend &1 &2 &3
        run_sqlplus(
            sql_file=SQL_FILE,
            dsn=dsn,
            user=ORACLE_USER,
            password=ORACLE_PASSWORD,
            args=[start_str, end_str, str(csv_path)],
            timeout=900,
        )

        if not csv_path.exists():
            raise RuntimeError(f"CSV manquant après spool: {csv_path}")

        res = analyze_csv_and_build_relations(csv_path)

        # stockage
        for groupe in results_by_type.keys():
            if "TYPE_RELATION" not in res.columns:
                continue
            subset = res[res["TYPE_RELATION"] == groupe].copy()
            subset = subset.drop(columns=["TYPE_RELATION"], errors="ignore")
            if not subset.empty:
                results_by_type[groupe].append((site_label, subset))

        # cleanup csv
        try:
            csv_path.unlink()
        except Exception:
            pass

        print(f"[INFO] Site {site_label}: OK")

    # Excel unique
    out_xlsx = OUTPUT_DIR / f"doublons_CCH_HTD_BRC_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx"

    with pd.ExcelWriter(out_xlsx, engine="openpyxl") as writer:
        for groupe in ["VRAI_DOUBLON", "DOUBLON_0_NUIT", "CONTIGUS", "CHEVAUCHEMENT"]:
            keep_ghm = (groupe == "CONTIGUS")

            if not results_by_type[groupe]:
                # placeholder
                if keep_ghm:
                    df_to_write = pd.DataFrame([{
                        "IPP": "N/A", "DATE_NAISSANCE": "N/A",
                        "NDA": "N/A", "UMA": "N/A", "GHM": "N/A",
                        "DATE_ENTREE": "N/A", "DATE_SORTIE": "N/A",
                        "SITE": "N/A", "COMMENTAIRE": ""
                    }])
                else:
                    df_to_write = pd.DataFrame([{
                        "IPP": "N/A", "DATE_NAISSANCE": "N/A",
                        "NDA": "N/A", "UMA": "N/A",
                        "DATE_ENTREE": "N/A", "DATE_SORTIE": "N/A",
                        "SITE": "N/A", "COMMENTAIRE": ""
                    }])
            else:
                blocks = []
                for site_label, subset in results_by_type[groupe]:
                    blocks.append(build_rows_instead_of_columns(subset, keep_ghm=keep_ghm, site_label=site_label))
                df_to_write = pd.concat(blocks, ignore_index=True)

            df_to_write.to_excel(writer, sheet_name=groupe, index=False)

            ws = writer.sheets[groupe]
            for i, col in enumerate(df_to_write.columns):
                max_len = max(df_to_write[col].astype(str).map(len).max(), len(col)) + 2
                ws.column_dimensions[ws.cell(row=1, column=i + 1).column_letter].width = max_len

    print(f"[INFO] Excel généré: {out_xlsx}")

    # Mail
    if not MAIL_CONFIG_PATH.exists():
        raise RuntimeError(f"Config mail introuvable: {MAIL_CONFIG_PATH}")

    cfg = json.loads(MAIL_CONFIG_PATH.read_text(encoding="utf-8"))
    mail_from = cfg["from"]
    to = cfg["to"]
    cc = cfg.get("cc", [])
    subject = cfg.get("subject", f"Liste doublons {start_str} -> {end_str}")
    body = cfg.get("body", f"Bonjour,\n\nVeuillez trouver ci-joint la liste des doublons/contigus/chevauchements.\nPériode : {start_str} -> {end_str}\n\nCordialement,\nDIM")

    send_mail(
        mail_from=mail_from,
        recipients=to,
        cc=cc,
        subject=subject,
        body=body,
        attachments=[str(out_xlsx)]
    )

    print("[INFO] Mail envoyé (ou DRY_RUN).")


if __name__ == "__main__":
    main()
