import pandas as pd
import numpy as np
from pathlib import Path

# =========================
# PARAMÈTRES
# =========================
PATH_ORBIS = Path("Liste des DMI codés.csv")
PATH_SIMPA = Path("DMI_NAS.csv")
PATH_SAG   = Path("Extraction_SAG.csv")
PATH_SEDI  = Path("Extraction DMI SEDITRACE 2025 CCH.xlsx")

OUT_BASE_DIM   = Path("BASE_DIM.xlsx")
OUT_MISSING    = Path("DMI_SEDITRACE_absents_DIM.xlsx")

# =========================
# HELPERS
# =========================
def read_csv_auto(path: Path) -> pd.DataFrame:
    # essaie encodages et séparateurs courants
    encodings = ["utf-8", "cp1252", "latin1"]
    seps = [None, ";", ",", "\t", "|"]
    last_err = None
    for enc in encodings:
        for sep in seps:
            try:
                if sep is None:
                    return pd.read_csv(path, sep=None, engine="python", encoding=enc)
                return pd.read_csv(path, sep=sep, engine="python", encoding=enc)
            except Exception as e:
                last_err = e
    raise RuntimeError(f"Impossible de lire {path}. Dernière erreur: {last_err}")

def norm_str(x: pd.Series) -> pd.Series:
    return (
        x.astype("string")
         .str.strip()
         .str.replace(r"\s+", " ", regex=True)
    )

def norm_ref(x: pd.Series) -> pd.Series:
    return norm_str(x).str.upper()

def norm_date_day(x: pd.Series) -> pd.Series:
    # Convertit tout en datetime puis tronque au jour (ignore l'heure)
    dt = pd.to_datetime(x, errors="coerce", dayfirst=True)
    return dt.dt.normalize()

def make_key(nip: pd.Series, date_pose: pd.Series, ref: pd.Series) -> pd.Series:
    nip_n = norm_str(nip)
    dt_n = norm_date_day(date_pose)
    ref_n = norm_ref(ref)
    # clé jour-strict
    return nip_n + "|" + dt_n.dt.strftime("%Y-%m-%d") + "|" + ref_n

def ensure_cols(df: pd.DataFrame, required: list, label: str):
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise KeyError(f"{label}: colonnes manquantes: {missing}")

def fmt_ddmmyyyy(dt_series: pd.Series) -> pd.Series:
    dt = pd.to_datetime(dt_series, errors="coerce", dayfirst=True)
    dt = dt.dt.normalize()
    return dt.dt.strftime("%d/%m/%Y")

# =========================
# 1) ORBIS -> standard
# =========================
orbis = read_csv_auto(PATH_ORBIS)

ORBI_COLS_REQ = [
    "IPP du patient",
    "Date de l’intervention",
    "Référence commerciale du DMI",
    "LPP",
    "Libellé du DMI du livret",
    "Nom du fournisseur",
    "N° de lot du DMI",
    "N° de série du DMI",
    "Date de codage du DMI",
]
ensure_cols(orbis, ORBI_COLS_REQ, "ORBIS")

orbis_std = pd.DataFrame({
    "NIP": orbis["IPP du patient"],
    "Date_pose": orbis["Date de l’intervention"],
    "Ref_commerciale": orbis["Référence commerciale du DMI"],
    "Code_LPP": orbis["LPP"],
    "Libelle_DMI": orbis["Libellé du DMI du livret"],
    "Fournisseur": orbis["Nom du fournisseur"],
    "No_lot": orbis["N° de lot du DMI"],
    "No_serie": orbis["N° de série du DMI"],
    "Date_saisie": orbis["Date de codage du DMI"],
    "SOURCE": "ORBIS"
})
orbis_std["Date_pose"] = norm_date_day(orbis_std["Date_pose"])
orbis_std["KEY"] = make_key(orbis_std["NIP"], orbis_std["Date_pose"], orbis_std["Ref_commerciale"])

# =========================
# 2) SAG -> standard
# =========================
sag = read_csv_auto(PATH_SAG)

SAG_COLS_REQ = [
    "NIP",
    "Date de réalisation",
    "Référence produit",
    "Code LPP",
    "Libellé produit",
    "Libellé forunisseur",  # attention: orthographe dans ton export
    "N° lot",
    "N°série",
]
ensure_cols(sag, SAG_COLS_REQ, "SAG")

sag_std = pd.DataFrame({
    "NIP": sag["NIP"],
    "Date_pose": sag["Date de réalisation"],
    "Ref_commerciale": sag["Référence produit"],
    "Code_LPP": sag["Code LPP"],
    "Libelle_DMI": sag["Libellé produit"],
    "Fournisseur": sag["Libellé forunisseur"],
    "No_lot": sag["N° lot"],
    "No_serie": sag["N°série"],
    "Date_saisie": pd.NA,
    "SOURCE": "SAG"
})
sag_std["Date_pose"] = norm_date_day(sag_std["Date_pose"])
sag_std["KEY"] = make_key(sag_std["NIP"], sag_std["Date_pose"], sag_std["Ref_commerciale"])

# =========================
# 3) SIMPA -> standard (minimal)
# =========================
simpa = read_csv_auto(PATH_SIMPA)

SIMPA_COLS_REQ = ["NIP", "DTEXEC", "CDREFF", "CDLPP"]
ensure_cols(simpa, SIMPA_COLS_REQ, "SIMPA")

simpa_std = pd.DataFrame({
    "NIP": simpa["NIP"],
    "Date_pose": simpa["DTEXEC"],
    "Ref_commerciale": simpa["CDREFF"],
    "Code_LPP": simpa["CDLPP"],
    "Libelle_DMI": pd.NA,
    "Fournisseur": pd.NA,
    "No_lot": pd.NA,
    "No_serie": pd.NA,
    "Date_saisie": pd.NA,
    "SOURCE": "SIMPA"
})
simpa_std["Date_pose"] = norm_date_day(simpa_std["Date_pose"])
simpa_std["KEY"] = make_key(simpa_std["NIP"], simpa_std["Date_pose"], simpa_std["Ref_commerciale"])

# =========================
# 4) BASE_DIM avec priorité ORBIS > SAG > SIMPA
# =========================
all_dim = pd.concat([orbis_std, sag_std, simpa_std], ignore_index=True)

# Enlève les clés invalides (NIP/date/ref manquants)
all_dim = all_dim.dropna(subset=["KEY"])

# Priorité: ORBIS(1) > SAG(2) > SIMPA(3)
priority_map = {"ORBIS": 1, "SAG": 2, "SIMPA": 3}
all_dim["PRIO"] = all_dim["SOURCE"].map(priority_map).astype("Int64")

# garde la meilleure ligne par KEY
all_dim_sorted = all_dim.sort_values(["KEY", "PRIO"], ascending=[True, True])
base_dim = all_dim_sorted.drop_duplicates(subset=["KEY"], keep="first").copy()

# colonnes de traçabilité
# (bonus) savoir dans quelles sources la KEY existe
sources_present = (
    all_dim.groupby("KEY")["SOURCE"]
           .apply(lambda s: "|".join(sorted(set(s.dropna().astype(str)))))
           .rename("Sources_presentes")
)
base_dim = base_dim.merge(sources_present, on="KEY", how="left")
base_dim = base_dim.rename(columns={"SOURCE": "Source_retendue"}).drop(columns=["PRIO"])

# format date pour export (dd/mm/yyyy)
base_dim["Date_pose"] = fmt_ddmmyyyy(base_dim["Date_pose"])

# export BASE_DIM.xlsx
with pd.ExcelWriter(OUT_BASE_DIM, engine="openpyxl") as writer:
    base_dim.to_excel(writer, sheet_name="BASE_DIM", index=False)

# =========================
# 5) SEDITRACE -> standard et anti-join (présent SEDI, absent DIM)
# =========================
sedi = pd.read_excel(PATH_SEDI)

SEDI_COLS_REQ = [
    "IPP",
    "Posé le",
    "Réference",
    "DATEHEURESAISIE",
    "Fournisseur",
    "N° de Lot",
    "LPPR",
    "Libellé",
    "Praticien",
    "Centre de coût",
    "Nbre",
    "PxTot",       # recette totale
    "PRIX_LPPR"    # prix unitaire (si besoin)
]
ensure_cols(sedi, SEDI_COLS_REQ, "SEDITRACE")

sedi_std = pd.DataFrame({
    "NIP": sedi["IPP"],
    "Date_pose": sedi["Posé le"],
    "Ref_commerciale": sedi["Réference"],
    "Date_saisie_SEDITRACE": sedi["DATEHEURESAISIE"],
    "Medecin_poseur": sedi["Praticien"],
    "Fournisseur": sedi["Fournisseur"],
    "No_lot": sedi["N° de Lot"],
    "No_serie": pd.NA,  # volontairement vide, comme demandé
    "Code_LPP": sedi["LPPR"],
    "Libelle_DMI": sedi["Libellé"],
    "UMA": sedi["Centre de coût"],
    "Quantite": sedi["Nbre"],
    "Prix_unitaire_LPPR": sedi["PRIX_LPPR"],
    "Prix_total": sedi["PxTot"],
})

sedi_std["Date_pose_dt"] = norm_date_day(sedi_std["Date_pose"])
sedi_std["KEY"] = make_key(sedi_std["NIP"], sedi_std["Date_pose_dt"], sedi_std["Ref_commerciale"])

# Anti-join
dim_keys = set(base_dim["KEY"].dropna().unique())
missing = sedi_std[~sedi_std["KEY"].isin(dim_keys)].copy()

# Normalise / affiche les dates au format dd/mm/yyyy
missing["Date_pose"] = fmt_ddmmyyyy(missing["Date_pose_dt"])
missing["Date_saisie_SEDITRACE"] = pd.to_datetime(missing["Date_saisie_SEDITRACE"], errors="coerce", dayfirst=True)\
    .dt.strftime("%d/%m/%Y %H:%M:%S")

# Colonnes finales demandées pour envoi aux services (10 champs)
missing_detail = missing[[
    "NIP",
    "Date_pose",
    "Medecin_poseur",
    "Date_saisie_SEDITRACE",
    "Fournisseur",
    "Ref_commerciale",
    "No_lot",
    "No_serie",
    "Code_LPP",
    "Libelle_DMI",
    "UMA",
    "Quantite",
    "Prix_unitaire_LPPR",
    "Prix_total",
]].copy()

# =========================
# 6) Synthèse UMA (pilotage PMSI)
# =========================
# Recette manquante: on prend Prix_total (PxTot) si numérique
missing_detail["Prix_total_num"] = pd.to_numeric(missing_detail["Prix_total"], errors="coerce")
missing_detail["Quantite_num"] = pd.to_numeric(missing_detail["Quantite"], errors="coerce")

uma_summary = (
    missing_detail
    .groupby("UMA", dropna=False)
    .agg(
        nb_dmi_manquants=("NIP", "size"),
        nb_patients=("NIP", pd.Series.nunique),
        quantite_totale=("Quantite_num", "sum"),
        recette_manquante_estimee=("Prix_total_num", "sum"),
    )
    .reset_index()
    .sort_values("recette_manquante_estimee", ascending=False)
)

# =========================
# 7) Export final
# =========================
with pd.ExcelWriter(OUT_MISSING, engine="openpyxl") as writer:
    missing_detail.to_excel(writer, sheet_name="DETAIL_DMI_MANQUANTS", index=False)
    uma_summary.to_excel(writer, sheet_name="SYNTHESE_UMA", index=False)

print("✅ OK")
print(f"- BASE_DIM: {OUT_BASE_DIM.resolve()}")
print(f"- DMI manquants: {OUT_MISSING.resolve()}")
