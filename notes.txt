# formatage_PMSIpilot.py
# Streamlit tool (Python 3.7) - MCO only for now
#
# Requirements (pin for Python 3.7):
#   streamlit
#   pandas==1.3.*
#   xlrd==1.2.0        # needed to read .xls
#   openpyxl==3.*      # needed to read .xlsx
#
# Run:
#   streamlit run formatage_PMSIpilot.py

import io
import re
import zipfile
from collections import Counter

import pandas as pd
import streamlit as st


# -----------------------------
# Helpers: parsing & validation
# -----------------------------

ZIP_MCO_RE = re.compile(r"^mco_(\d{3})_(\d{6})_in\.zip$", re.IGNORECASE)
XLS_MCO_RE = re.compile(
    r"^CORRESPONDANCE_UM_MCO_SITE_(\d{5})_APHP_(\d{6})\.(xls|xlsx)$",
    re.IGNORECASE,
)

RSS_RE = re.compile(r"^(\d{9})\.(\d{4})\.(\d{2})\.rss\.ini\.txt$", re.IGNORECASE)
ANO_RE = re.compile(r"^(\d{9})\.(\d{4})\.(\d{2})\.ano\.txt$", re.IGNORECASE)


def parse_zip_name(zip_filename: str):
    m = ZIP_MCO_RE.match(zip_filename or "")
    if not m:
        return None
    return {"domain": "MCO", "site3": m.group(1), "yyyymm": m.group(2)}


def parse_excel_name(excel_filename: str):
    m = XLS_MCO_RE.match(excel_filename or "")
    if not m:
        return None
    return {"domain": "MCO", "site5": m.group(1), "yyyymm": m.group(2), "ext": m.group(3).lower()}


def site3_to_site5(site3: str) -> str:
    # "021" -> "00021"
    return (site3 or "").zfill(5)


def safe_decode_singlebyte(data: bytes) -> str:
    # fixed-width files → use latin-1 to preserve 1 byte = 1 char mapping
    return data.decode("latin-1", errors="replace")


def safe_encode_singlebyte(text: str) -> bytes:
    return text.encode("latin-1", errors="replace")


# -----------------------------
# Excel mapping (UM APHP -> UM SITE)
# -----------------------------

def read_um_mapping(excel_bytes: bytes, excel_filename: str) -> pd.DataFrame:
    ext = (excel_filename or "").lower().split(".")[-1]
    bio = io.BytesIO(excel_bytes)

    if ext == "xls":
        # IMPORTANT: needs xlrd==1.2.0
        df = pd.read_excel(bio, engine="xlrd", skiprows=2)
    else:
        # xlsx
        df = pd.read_excel(bio, engine="openpyxl", skiprows=2)

    df = df.dropna(how="all")

    # Normalize column names (strip spaces)
    cols = {c: str(c).strip() for c in df.columns}
    df = df.rename(columns=cols)

    # Expect URM_APHP and URM_SITE (as in your R files)
    # If the case/spacing differs, try to find them.
    col_upper = {c.upper().replace(" ", "").replace("-", "_"): c for c in df.columns}
    urm_aphp_col = col_upper.get("URM_APHP") or col_upper.get("URMAPHP")
    urm_site_col = col_upper.get("URM_SITE") or col_upper.get("URMSITE")

    if not urm_aphp_col or not urm_site_col:
        raise ValueError(
            "Colonnes attendues non trouvées dans l'Excel. "
            "Il faut des colonnes 'URM_APHP' et 'URM_SITE'."
        )

    out = df[[urm_aphp_col, urm_site_col]].copy()
    out.columns = ["URM_APHP", "URM_SITE"]

    # Keep strings as-is (no padding/correction here), just strip
    out["URM_APHP"] = out["URM_APHP"].astype(str).str.strip()
    out["URM_SITE"] = out["URM_SITE"].astype(str).str.strip()

    # Drop empty keys
    out = out[(out["URM_APHP"].notna()) & (out["URM_APHP"] != "")]

    # Force 1->1 mapping (keep first occurrence)
    out = out.drop_duplicates(subset=["URM_APHP"], keep="first")

    return out


# -----------------------------
# MCO RSS processing (UM replacement)
# -----------------------------

def process_rss_text_mco(rss_bytes: bytes, mapping: dict, start_1_based: int, length: int):
    """
    Replace UM APHP at fixed position (start_1_based, length) with UM_SITE.

    Returns: (corrected_bytes, stats_dict)
    """
    text = safe_decode_singlebyte(rss_bytes)
    lines = text.splitlines(True)  # keep line endings

    start0 = start_1_based - 1
    end0 = start0 + length

    total = 0
    matched = 0
    missing_keys = Counter()

    out_lines = []
    for line in lines:
        # keep newline as part of line; slicing still works
        if len(line) < end0:
            out_lines.append(line)
            continue

        seg = line[start0:end0]
        key = seg.replace(" ", "")
        total += 1

        if key in mapping:
            rep = mapping[key]
            # keep fixed width = length, pad right with spaces
            rep_fixed = (rep + (" " * length))[:length]
            new_line = line[:start0] + rep_fixed + line[end0:]
            out_lines.append(new_line)
            matched += 1
        else:
            # Keep original segment to avoid corrupting files, but report it
            out_lines.append(line)
            missing_keys[key] += 1

    corrected = "".join(out_lines)
    stats = {
        "total_records_checked": total,
        "matched": matched,
        "match_rate": (matched / total) if total else 0.0,
        "missing_top10": missing_keys.most_common(10),
        "missing_unique": len(missing_keys),
    }
    return safe_encode_singlebyte(corrected), stats


# -----------------------------
# ANOHOSP -> VIDHOSP (Julia logic translated)
# -----------------------------

def ss_1based(s: str, a: int, b: int) -> str:
    """1-based inclusive substring, safe with padding."""
    if a < 1 or b < a:
        return ""
    # pad if needed
    if len(s) < b:
        s = s + (" " * (b - len(s)))
    return s[a - 1:b]


def fix_anohosp_v014(ano: str) -> str:
    # Julia: string(SubString(ano,1,113),SubString(ano,116))
    # remove positions 114-115 (1-based)
    if len(ano) < 116:
        ano = ano + (" " * (116 - len(ano)))
    return ano[:113] + ano[115:]


def ano_format(ano: str) -> str:
    return "V" + ss_1based(ano, 17, 19)


def ano_regime(ano: str) -> str:
    return ss_1based(ano, 141, 142)


def ano_gestion(ano: str) -> str:
    return ss_1based(ano, 143, 144)


def ano_ddn(ano: str) -> str:
    return ss_1based(ano, 248, 255)


def ano_nas(ano: str) -> str:
    return ss_1based(ano, 41, 60)


def ano_exo_a_montant_amc(ano: str) -> str:
    return ss_1based(ano, 145, 247)


def ano_participation_a_centre_gestion(ano: str) -> str:
    return ss_1based(ano, 257, 277)


def ano_confirmation_pc_a_ipp(ano: str) -> str:
    return ss_1based(ano, 278, 420)


def ano_sexe(ano: str) -> str:
    x = ss_1based(ano, 256, 256)
    return x if x in ("1", "2") else "1"


def ano_annee_ddn(ano: str) -> str:
    return ss_1based(ano, 254, 255)


def ano_mois_ddn(ano: str) -> str:
    return ss_1based(ano, 250, 251)


def ano_ipp_cropped(ano: str) -> str:
    return ss_1based(ano, 403, 410)


def ano_cle(num_secu_str: str) -> str:
    # Julia: 97 - parse(Int,num_secu) % 97
    # If parsing fails, return "00" (and we will report)
    try:
        n = int(num_secu_str)
        cle = 97 - (n % 97)
        return str(cle).zfill(2)
    except Exception:
        return "00"


def anohosp_to_vidhosp_lines(finess: str, ano_bytes: bytes):
    """
    Convert anohosp file content into vidhosp lines.
    Returns (vid_bytes, stats_dict)
    """
    text = safe_decode_singlebyte(ano_bytes)
    lines = [ln.rstrip("\r\n") for ln in text.splitlines()]

    kept = 0
    produced = 0
    skipped_format = Counter()
    parse_errors = 0

    out = []

    filler30 = " " * 30
    art51 = " "  # filler(1)

    for ano in lines:
        if not ano:
            continue

        fmt = ano_format(ano)
        kept += 1

        if fmt not in ("V013", "V014", "V015"):
            skipped_format[fmt] += 1
            continue

        # TEMP anohosp v014 simpa: keep V013 as-is, fix others
        if fmt != "V013":
            ano = fix_anohosp_v014(ano)

        fake_num_am = (
            ano_sexe(ano)
            + ano_annee_ddn(ano)
            + ano_mois_ddn(ano)
            + ano_ipp_cropped(ano)
        )

        cle = ano_cle(fake_num_am)
        regime = ano_regime(ano)
        gestion = ano_gestion(ano)
        ddn = ano_ddn(ano)
        sexe = ano_sexe(ano)
        nas = ano_nas(ano)

        exo = ano_exo_a_montant_amc(ano)
        part = ano_participation_a_centre_gestion(ano)
        conf = ano_confirmation_pc_a_ipp(ano)

        # If fake_num_am is not numeric, cle="00" and it may be wrong -> report
        if cle == "00":
            parse_errors += 1

        if fmt == "V013":
            raw = (
                fake_num_am
                + cle
                + regime
                + gestion
                + ddn
                + sexe
                + nas
                + fmt
                + finess
                + fake_num_am
                + cle
                + exo
                + part
                + filler30
                + conf
            )
        else:
            raw = (
                fake_num_am
                + cle
                + regime
                + gestion
                + ddn
                + sexe
                + nas
                + fmt
                + finess
                + fake_num_am
                + cle
                + exo
                + part
                + filler30
                + conf
                # new in 2020 / v014+
                + fake_num_am
                + cle
                + art51
            )

        out.append(raw)
        produced += 1

    vid_text = "\n".join(out) + ("\n" if out else "")
    stats = {
        "anohosp_lines_total": len(lines),
        "anohosp_nonempty_seen": kept,
        "vidhosp_produced": produced,
        "skipped_format_top10": skipped_format.most_common(10),
        "parse_errors_cle00": parse_errors,
    }
    return safe_encode_singlebyte(vid_text), stats


# -----------------------------
# Domain detection
# -----------------------------

def detect_domain(zip_filename: str, zip_namelist):
    # 1) By zip filename
    zn = (zip_filename or "").lower()
    if zn.startswith("mco_"):
        return "MCO"
    if zn.startswith("smr_") or zn.startswith("ssr_"):
        return "SMR"
    if zn.startswith("psy_"):
        return "PSY"

    # 2) By content
    lowered = [n.lower() for n in zip_namelist]
    if any(n.endswith(".rss.ini.txt") for n in lowered):
        return "MCO"
    if any(n.endswith(".rhs.ini.txt") for n in lowered):
        return "SMR"
    # PSY patterns to add later
    return "UNKNOWN"


# -----------------------------
# Main MCO processing (zip in -> zip out)
# -----------------------------

def process_zip_mco(zip_bytes: bytes, zip_filename: str, excel_bytes: bytes, excel_filename: str,
                    keep_original_rss: bool = True,
                    rss_um_start_1based: int = 87,
                    rss_um_length: int = 4,
                    min_match_rate: float = 0.90):
    report_lines = []

    # Validate names if possible
    zip_meta = parse_zip_name(zip_filename)
    xls_meta = parse_excel_name(excel_filename)

    if zip_meta:
        report_lines.append(f"ZIP détecté: domaine={zip_meta['domain']} site={zip_meta['site3']} periode={zip_meta['yyyymm']}")
    else:
        report_lines.append("ZIP: nom non standard (pas de validation site/période via le nom).")

    if xls_meta:
        report_lines.append(f"EXCEL détecté: domaine={xls_meta['domain']} site={xls_meta['site5']} periode={xls_meta['yyyymm']}")
    else:
        report_lines.append("EXCEL: nom non standard (pas de validation site/période via le nom).")

    if zip_meta and xls_meta:
        expected_site5 = site3_to_site5(zip_meta["site3"])
        if xls_meta["site5"] != expected_site5 or xls_meta["yyyymm"] != zip_meta["yyyymm"]:
            raise ValueError(
                f"Incohérence ZIP/EXCEL: ZIP site={zip_meta['site3']} periode={zip_meta['yyyymm']} "
                f"mais EXCEL site={xls_meta['site5']} periode={xls_meta['yyyymm']} "
                f"(attendu site5={expected_site5})."
            )

    # Read mapping
    um_df = read_um_mapping(excel_bytes, excel_filename)
    mapping = dict(zip(um_df["URM_APHP"].tolist(), um_df["URM_SITE"].tolist()))
    report_lines.append(f"Mapping UM: {len(mapping)} clés (URM_APHP -> URM_SITE) après déduplication 1->1.")

    # Open input zip
    zin = zipfile.ZipFile(io.BytesIO(zip_bytes), "r")
    names = zin.namelist()

    rss_files = [n for n in names if RSS_RE.match(n.split("/")[-1])]  # allow subfolders
    ano_files = [n for n in names if ANO_RE.match(n.split("/")[-1])]

    report_lines.append(f"Fichiers RSS trouvés: {len(rss_files)}")
    report_lines.append(f"Fichiers ANO trouvés: {len(ano_files)}")

    # Build output zip in memory
    zout_buffer = io.BytesIO()
    with zipfile.ZipFile(zout_buffer, "w", compression=zipfile.ZIP_DEFLATED) as zout:
        # Copy everything as-is
        for n in names:
            zout.writestr(n, zin.read(n))

        # Add corrected RSS (keep original)
        for rss_name in rss_files:
            base = rss_name.split("/")[-1]
            m = RSS_RE.match(base)
            finess, yyyy, mm = m.group(1), m.group(2), m.group(3)

            rss_bytes_in = zin.read(rss_name)
            corrected_bytes, stats = process_rss_text_mco(
                rss_bytes_in, mapping,
                start_1_based=rss_um_start_1based,
                length=rss_um_length
            )

            report_lines.append(
                f"RSS {base}: checked={stats['total_records_checked']} matched={stats['matched']} "
                f"rate={stats['match_rate']:.3f} missing_unique={stats['missing_unique']}"
            )
            if stats["missing_top10"]:
                report_lines.append(f"  Top missing keys (up to 10): {stats['missing_top10']}")

            # Guardrail
            if stats["total_records_checked"] > 0 and stats["match_rate"] < min_match_rate:
                raise ValueError(
                    f"Taux de match UM trop faible sur {base}: {stats['match_rate']:.3f} < {min_match_rate:.2f}. "
                    f"Probable mauvais fichier de correspondance."
                )

            # Write corrected alongside original
            prefix = rss_name[: -len(base)]
            corrected_name = prefix + base.replace(".rss.ini.txt", ".rss.ini-corrige.txt")
            zout.writestr(corrected_name, corrected_bytes)

        # Add VIDHOSP for each ANO
        for ano_name in ano_files:
            base = ano_name.split("/")[-1]
            m = ANO_RE.match(base)
            finess, yyyy, mm = m.group(1), m.group(2), m.group(3)

            ano_bytes_in = zin.read(ano_name)
            vid_bytes, vid_stats = anohosp_to_vidhosp_lines(finess=finess, ano_bytes=ano_bytes_in)

            report_lines.append(
                f"ANO {base}: nonempty_seen={vid_stats['anohosp_nonempty_seen']} "
                f"vidhosp_produced={vid_stats['vidhosp_produced']} parse_errors_cle00={vid_stats['parse_errors_cle00']}"
            )
            if vid_stats["skipped_format_top10"]:
                report_lines.append(f"  Formats ignorés (top): {vid_stats['skipped_format_top10']}")

            prefix = ano_name[: -len(base)]
            vid_name = prefix + f"{finess}.{yyyy}.{mm}.vidhosp.txt"
            zout.writestr(vid_name, vid_bytes)

        # Add report
        report_txt = "\n".join(report_lines) + "\n"
        zout.writestr("PMSIPilot_report.txt", safe_encode_singlebyte(report_txt))

    zin.close()
    zout_buffer.seek(0)
    return zout_buffer.getvalue(), "\n".join(report_lines)


# -----------------------------
# Streamlit UI
# -----------------------------

def main():
    st.set_page_config(page_title="Formatage PMSIPilot", layout="centered")
    st.title("Formatage PMSIPilot — MCO (V1)")

    st.markdown(
        "- Dépose un **ZIP MCO** (`mco_XXX_YYYYMM_in.zip`) + un **Excel de correspondance UM** (`.xls` ou `.xlsx`).\n"
        "- Le ZIP de sortie contient **tous les fichiers d'origine** + `rss.ini-corrige.txt` + `vidhosp.txt` + un rapport."
    )

    mode = st.selectbox("Type", ["Auto", "MCO", "SMR", "PSY"], index=0)

    col1, col2 = st.columns(2)
    with col1:
        uploaded_zip = st.file_uploader("ZIP d'entrée", type=["zip"])
    with col2:
        uploaded_excel = st.file_uploader("Excel correspondance UM", type=["xls", "xlsx"])

    keep_original_rss = st.checkbox("Conserver le RSS original (ajouter un RSS corrigé)", value=True)
    min_match_rate = st.slider("Seuil minimum de match UM (anti-mauvais Excel)", 0.0, 1.0, 0.90, 0.01)

    if uploaded_zip and uploaded_excel:
        zip_bytes = uploaded_zip.read()
        excel_bytes = uploaded_excel.read()

        # Quick open to list entries
        try:
            with zipfile.ZipFile(io.BytesIO(zip_bytes), "r") as z:
                namelist = z.namelist()
        except Exception as e:
            st.error(f"ZIP invalide: {e}")
            return

        detected = detect_domain(uploaded_zip.name, namelist)
        chosen = detected if mode == "Auto" else mode

        st.write(f"Détection: **{detected}** — Mode choisi: **{chosen}**")

        if chosen in ("SMR", "PSY"):
            st.warning(f"{chosen} n'est pas encore implémenté. (MCO seulement pour l'instant)")
            return
        if chosen not in ("MCO",):
            st.error("Impossible de détecter le domaine. Force MCO via le menu si besoin.")
            return

        if st.button("Lancer le traitement", type="primary"):
            try:
                out_zip_bytes, report = process_zip_mco(
                    zip_bytes=zip_bytes,
                    zip_filename=uploaded_zip.name,
                    excel_bytes=excel_bytes,
                    excel_filename=uploaded_excel.name,
                    keep_original_rss=keep_original_rss,
                    rss_um_start_1based=87,
                    rss_um_length=4,
                    min_match_rate=float(min_match_rate),
                )
            except Exception as e:
                st.error(str(e))
                return

            st.success("Traitement terminé.")
            st.text_area("Rapport", report, height=280)

            out_name = (uploaded_zip.name or "mco_in.zip").replace("_in.zip", "_out.zip")
            st.download_button(
                label="Télécharger le ZIP formaté",
                data=out_zip_bytes,
                file_name=out_name,
                mime="application/zip",
            )

    else:
        st.info("Téléverse un ZIP + un Excel pour démarrer.")


if __name__ == "__main__":
    main()
