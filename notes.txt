sql :
whenever sqlerror exit 1
whenever oserror exit 2
SET LINESIZE 32767
SET HEADING ON
SET COLSEP ";"
SET PAGESIZE 0
SET FEEDBACK OFF
SET TRIMSPOOL ON
SET WRAP OFF
SET TERMOUT OFF

-- Param√®tres attendus:
--   &1 = date_debut (DD/MM/YYYY)
--   &2 = date_fin   (DD/MM/YYYY)
--   &3 = chemin spool (ex: data/resultat_CCH.csv)

SPOOL &3

SELECT DISTINCT
       S.KYNOIP AS IPP,
       S.DANAIS AS DATE_NAISSANCE,
       E.KYNODA AS NDA,
       E.CDURM_P AS UMA,
       R.CDGHM AS GHM,
       TO_CHAR(E.D8EEUE, 'DD/MM/YYYY') AS DATE_ENTREE,
       TO_CHAR(E.D8SOUE, 'DD/MM/YYYY') AS DATE_SORTIE
FROM EPI E, SDO S, RSS R
WHERE E.KYNODA = S.KYNODA
  AND E.KYNODA = R.NODA(+)
  AND E.D8EEUE >= TO_DATE('&1','DD/MM/YYYY')
  AND E.D8EEUE <= TO_DATE('&2','DD/MM/YYYY')
  AND E.CHPPMSI = 'CS'
ORDER BY S.KYNOIP;

SPOOL OFF
EXIT;

code:
import streamlit as st
import pandas as pd
import subprocess
from datetime import datetime, date, timedelta
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parents[1]
SQL_DIR = BASE_DIR / "SQL"
DATA_DIR = BASE_DIR / "data"

TEST_SQL = SQL_DIR / "test_connexion.sql"
MAIN_SQL = SQL_DIR / "contigus.sql"

# Liste des TNS des sites pour SIMPA (m√™me pattern que ton autre outil)
TNS_MAP = {
    "CCH": "//o-simpa-b1.cch.aphp.fr:10805/SIP1CCH",
    "BRC": "//o-simpa-b1.brc.aphp.fr:8105/SIP1BRC",
    "HTD": "//o-simpa-b1.htd.aphp.fr:8855/SIP1HTD"
}

def first_day_of_month(d: date) -> date:
    return d.replace(day=1)

def add_months(d: date, months: int) -> date:
    # petit helper sans d√©pendances externes
    y = d.year + (d.month - 1 + months) // 12
    m = (d.month - 1 + months) % 12 + 1
    day = min(d.day, [31,
                      29 if (y % 4 == 0 and (y % 100 != 0 or y % 400 == 0)) else 28,
                      31, 30, 31, 30, 31, 31, 30, 31, 30, 31][m - 1])
    return date(y, m, day)

def last_day_of_month(d: date) -> date:
    return add_months(first_day_of_month(d), 1) - timedelta(days=1)

def compute_window_m2_to_end_m1(today: date):
    # fen√™tre: du 1er jour de M-2 au dernier jour de M-1
    m1 = add_months(today, -1)
    m2 = add_months(today, -2)
    start = first_day_of_month(m2)
    end = last_day_of_month(m1)
    return start, end

def test_oracle_connection(user, password, tns_alias):
    cmd = ["sqlplus", "-S", f"{user}/{password}@{tns_alias}", f"@{TEST_SQL}"]
    try:
        result = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=10
        )
        if result.returncode != 0:
            return False, (result.stderr or result.stdout)
        return True, ""
    except subprocess.TimeoutExpired:
        return False, "Connexion Oracle: timeout"
    except Exception as e:
        return False, f"Connexion Oracle: erreur inattendue: {e}"

def run_main_sql(user, password, tns_alias, date_debut_str, date_fin_str, spool_path_str):
    # contigus.sql attend &1 &2 &3
    cmd = [
        "sqlplus", "-S",
        f"{user}/{password}@{tns_alias}",
        f"@{MAIN_SQL}",
        date_debut_str,
        date_fin_str,
        spool_path_str
    ]
    result = subprocess.run(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )
    return result

def analyze_csv_and_build_relations(csv_path: Path):
    # --- COPIE de ta logique actuelle (inchang√©e) ---
    df = pd.read_csv(
        csv_path,
        sep=";",
        skip_blank_lines=True,
        dtype=str,
        encoding="latin-1"
    )

    df = df.dropna(how="all")
    df.columns = [col.strip().upper() for col in df.columns]

    expected_cols = ["IPP", "DATE_NAISSANCE", "NDA", "UMA", "GHM", "DATE_ENTREE", "DATE_SORTIE"]
    if df.shape[1] >= len(expected_cols):
        df.columns = expected_cols + [f"COL_EXTRA_{i}" for i in range(df.shape[1] - len(expected_cols))]

    df["IPP"] = df["IPP"].astype(str).str.strip()
    df["UMA"] = df["UMA"].fillna("").astype(str).str.strip()
    df["NDA"] = df["NDA"].astype(str).str.strip()
    df["GHM"] = df["GHM"].fillna("").astype(str).str.strip()

    df["DATE_ENTREE"] = pd.to_datetime(df["DATE_ENTREE"], dayfirst=True, errors="coerce")
    df["DATE_SORTIE"] = pd.to_datetime(df["DATE_SORTIE"], dayfirst=True, errors="coerce")

    df = df[~df["UMA"].isin(["540", "543"])]

    df = df.dropna(subset=["DATE_ENTREE", "DATE_SORTIE", "UMA", "IPP"])
    df = df.sort_values(["IPP", "DATE_ENTREE", "DATE_SORTIE"]).reset_index(drop=True)

    df_base = df.drop_duplicates(
        subset=["IPP", "NDA", "UMA", "DATE_ENTREE", "DATE_SORTIE"],
        keep="first"
    ).copy()

    df_contigus = df.copy()

    rows = []

    def stay_key(ipp, nda, uma, de, ds):
        return (ipp, nda, uma, de, ds)

    stay_to_ghm = {}
    stay_to_ghm_full = {}
    for _, r in df_contigus.iterrows():
        ipp = r["IPP"]
        nda = r["NDA"]
        uma = r["UMA"]
        de = r["DATE_ENTREE"]
        ds = r["DATE_SORTIE"]
        ghm = str(r["GHM"]).strip()

        k = stay_key(ipp, nda, uma, de, ds)
        if ghm and ghm.upper() not in ("NAN", "NONE") and len(ghm) >= 5:
            root = ghm[:5]
            stay_to_ghm.setdefault(k, set()).add(root)
            stay_to_ghm_full.setdefault(k, {})
            stay_to_ghm_full[k].setdefault(root, ghm)

    assigned = set()

    def pair_id(k1, k2):
        return tuple(sorted([k1, k2]))

    def get_dn(g, i):
        try:
            return g.loc[i, "DATE_NAISSANCE"]
        except Exception:
            return ""

    # 1) VRAI_DOUBLON + DOUBLON_0_NUIT + candidats chevauchement
    for ipp, group in df_base.groupby("IPP", sort=False):
        g = group.reset_index(drop=True)
        n = len(g)

        for i in range(n):
            nda1, uma1, de1, ds1 = g.loc[i, ["NDA", "UMA", "DATE_ENTREE", "DATE_SORTIE"]]
            k1 = stay_key(ipp, nda1, uma1, de1, ds1)

            j = i + 1
            while j < n:
                de2 = g.loc[j, "DATE_ENTREE"]
                if pd.isna(de2):
                    j += 1
                    continue
                if de2 > ds1 and de2 != ds1:
                    break

                nda2, uma2, ds2 = g.loc[j, ["NDA", "UMA", "DATE_SORTIE"]]
                k2 = stay_key(ipp, nda2, uma2, de2, ds2)

                if nda1 == nda2:
                    j += 1
                    continue

                pid = pair_id(k1, k2)
                if pid in assigned:
                    j += 1
                    continue

                rel = None
                if (uma1 == uma2) and (de1 == de2) and (ds1 == ds2):
                    rel = "VRAI_DOUBLON"
                elif (uma1 != uma2) and (de1 == de2) and (ds1 == ds2):
                    rel = "DOUBLON_0_NUIT"
                elif (uma1 != uma2) and (de1 < ds2) and (ds1 > de2):
                    rel = "CHEVAUCHEMENT_CANDIDAT"
                else:
                    rel = None

                if rel and rel != "CHEVAUCHEMENT_CANDIDAT":
                    assigned.add(pid)
                    rows.append({
                        "IPP": ipp,
                        "DATE_NAISSANCE": get_dn(g, i),

                        "NDA1": nda1, "UMA1": uma1, "GHM1": "",
                        "DATEE1": de1, "DATES1": ds1,

                        "NDA2": nda2, "UMA2": uma2, "GHM2": "",
                        "DATEE2": de2, "DATES2": ds2,

                        "TYPE_RELATION": rel
                    })

                j += 1

    # 2) CONTIGUS
    for ipp, group in df_base.groupby("IPP", sort=False):
        g = group.reset_index(drop=True)
        n = len(g)

        for i in range(n):
            nda1, uma1, de1, ds1 = g.loc[i, ["NDA", "UMA", "DATE_ENTREE", "DATE_SORTIE"]]
            k1 = stay_key(ipp, nda1, uma1, de1, ds1)

            j = i + 1
            while j < n:
                de2 = g.loc[j, "DATE_ENTREE"]
                if pd.isna(de2):
                    j += 1
                    continue
                if de2 > ds1 and de2 != ds1:
                    break

                nda2, uma2, ds2 = g.loc[j, ["NDA", "UMA", "DATE_SORTIE"]]
                k2 = stay_key(ipp, nda2, uma2, de2, ds2)

                if nda1 == nda2:
                    j += 1
                    continue

                pid = pair_id(k1, k2)
                if pid in assigned:
                    j += 1
                    continue

                is_contig = (ds1 == de2) or (ds2 == de1)
                if not is_contig or uma1 == uma2:
                    j += 1
                    continue

                roots1 = stay_to_ghm.get(k1, set())
                roots2 = stay_to_ghm.get(k2, set())
                common = roots1.intersection(roots2)

                if common:
                    root = sorted(list(common))[0]
                    ghm1 = stay_to_ghm_full.get(k1, {}).get(root, "")
                    ghm2 = stay_to_ghm_full.get(k2, {}).get(root, "")

                    assigned.add(pid)
                    rows.append({
                        "IPP": ipp,
                        "DATE_NAISSANCE": get_dn(g, i),

                        "NDA1": nda1, "UMA1": uma1, "GHM1": ghm1,
                        "DATEE1": de1, "DATES1": ds1,

                        "NDA2": nda2, "UMA2": uma2, "GHM2": ghm2,
                        "DATEE2": de2, "DATES2": ds2,

                        "TYPE_RELATION": "CONTIGUS"
                    })

                j += 1

    # 3) CHEVAUCHEMENT (apr√®s exclusions)
    for ipp, group in df_base.groupby("IPP", sort=False):
        g = group.reset_index(drop=True)
        n = len(g)

        for i in range(n):
            nda1, uma1, de1, ds1 = g.loc[i, ["NDA", "UMA", "DATE_ENTREE", "DATE_SORTIE"]]
            k1 = stay_key(ipp, nda1, uma1, de1, ds1)

            j = i + 1
            while j < n:
                de2 = g.loc[j, "DATE_ENTREE"]
                if pd.isna(de2):
                    j += 1
                    continue
                if de2 > ds1 and de2 != ds1:
                    break

                nda2, uma2, ds2 = g.loc[j, ["NDA", "UMA", "DATE_SORTIE"]]
                k2 = stay_key(ipp, nda2, uma2, de2, ds2)

                if nda1 == nda2:
                    j += 1
                    continue

                pid = pair_id(k1, k2)
                if pid in assigned:
                    j += 1
                    continue

                if (uma1 != uma2) and (de1 < ds2) and (ds1 > de2):
                    assigned.add(pid)
                    rows.append({
                        "IPP": ipp,
                        "DATE_NAISSANCE": get_dn(g, i),

                        "NDA1": nda1, "UMA1": uma1, "GHM1": "",
                        "DATEE1": de1, "DATES1": ds1,

                        "NDA2": nda2, "UMA2": uma2, "GHM2": "",
                        "DATEE2": de2, "DATES2": ds2,

                        "TYPE_RELATION": "CHEVAUCHEMENT"
                    })

                j += 1

    res = pd.DataFrame(rows).drop_duplicates().reset_index(drop=True)
    res["PAIR_ID"] = res.index + 1
    return res

def build_rows_instead_of_columns(df_rel, keep_ghm: bool, site_label: str):
    left_cols = {
        "IPP": "IPP",
        "DATE_NAISSANCE": "DATE_NAISSANCE",
        "NDA": "NDA1",
        "UMA": "UMA1",
        "DATE_ENTREE": "DATEE1",
        "DATE_SORTIE": "DATES1",
        "PAIR_ID": "PAIR_ID",
    }
    if keep_ghm:
        left_cols["GHM"] = "GHM1"

    df1 = df_rel[list(left_cols.values())].copy()
    df1.columns = list(left_cols.keys())
    df1["LIGNE"] = 1

    right_cols = {
        "IPP": "IPP",
        "DATE_NAISSANCE": "DATE_NAISSANCE",
        "NDA": "NDA2",
        "UMA": "UMA2",
        "DATE_ENTREE": "DATEE2",
        "DATE_SORTIE": "DATES2",
        "PAIR_ID": "PAIR_ID",
    }
    if keep_ghm:
        right_cols["GHM"] = "GHM2"

    df2 = df_rel[list(right_cols.values())].copy()
    df2.columns = list(right_cols.keys())
    df2["LIGNE"] = 2

    out = pd.concat([df1, df2], ignore_index=True)

    for c in ["DATE_ENTREE", "DATE_SORTIE"]:
        out[c] = pd.to_datetime(out[c], errors="coerce").dt.strftime("%d/%m/%Y")

    # Ajout colonne SITE avant COMMENTAIRE
    out["SITE"] = site_label
    out["COMMENTAIRE"] = ""

    if keep_ghm:
        out = out[["IPP", "DATE_NAISSANCE", "NDA", "UMA", "GHM",
                   "DATE_ENTREE", "DATE_SORTIE", "SITE", "COMMENTAIRE"]]
    else:
        out = out[["IPP", "DATE_NAISSANCE", "NDA", "UMA",
                   "DATE_ENTREE", "DATE_SORTIE", "SITE", "COMMENTAIRE"]]

    out = out.sort_values(["IPP"]).reset_index(drop=True)
    return out

def main():
    st.title("D√©tection des Doublons, Contigus et Chevauchements (CCH / HTD / BRC)")
    st.markdown(
        "Cet outil interroge les 3 sites, analyse les s√©jours et g√©n√®re un fichier Excel unique."
    )

    start, end = compute_window_m2_to_end_m1(date.today())
    start_str = start.strftime("%d/%m/%Y")
    end_str = end.strftime("%d/%m/%Y")
    st.info(f"P√©riode analys√©e : du **{start_str}** au **{end_str}** (M-2 ‚Üí fin M-1)")

    with st.form("connexion"):
        st.subheader("Connexion Oracle")
        user = st.text_input("üë§ Nom d'utilisateur Oracle :", value="")
        password = st.text_input("üîë Mot de passe :", type="password")
        submitted = st.form_submit_button("üöÄ Lancer le traitement (3 sites)")

    if not submitted:
        return

    if not user or not password:
        st.error("Veuillez renseigner vos identifiants Oracle.")
        st.stop()

    # R√©sultat global: dict relation_type -> list of dataframes
    results_by_type = {k: [] for k in ["VRAI_DOUBLON", "DOUBLON_0_NUIT", "CONTIGUS", "CHEVAUCHEMENT"]}

    progress = st.progress(0)
    step = 0
    total_steps = len(TNS_MAP) * 3  # test + run + analyse

    for site_label, tns_alias in TNS_MAP.items():
        step += 1
        progress.progress(min(step / total_steps, 1.0))
        st.write(f"üîπ Site **{site_label}** ‚Äî test connexion...")

        ok, msg = test_oracle_connection(user, password, tns_alias)
        if not ok:
            st.error(f"Connexion Oracle √©chou√©e sur {site_label}")
            if msg:
                st.code(msg)
            st.stop()

        # spool sp√©cifique au site
        csv_path = DATA_DIR / f"resultat_{site_label}.csv"
        try:
            if csv_path.exists():
                csv_path.unlink()
        except Exception:
            pass

        step += 1
        progress.progress(min(step / total_steps, 1.0))
        st.write(f"üîπ Site **{site_label}** ‚Äî ex√©cution SQL*Plus...")

        result = run_main_sql(user, password, tns_alias, start_str, end_str, str(csv_path))
        if result.returncode != 0:
            st.error(f"Erreur SQL*Plus sur {site_label}")
            st.code(result.stderr or result.stdout)
            st.stop()

        if not csv_path.exists():
            st.error(f"Le fichier CSV n'existe pas apr√®s ex√©cution SQL sur {site_label} ({csv_path.name}).")
            st.code(result.stderr or result.stdout)
            st.stop()

        step += 1
        progress.progress(min(step / total_steps, 1.0))
        st.write(f"üîπ Site **{site_label}** ‚Äî analyse des s√©jours...")

        try:
            res = analyze_csv_and_build_relations(csv_path)
        except Exception as e:
            st.error(f"Erreur analyse sur {site_label} : {e}")
            st.stop()

        # Stockage par type
        for groupe in results_by_type.keys():
            subset = res[res["TYPE_RELATION"] == groupe].copy()
            subset = subset.drop(columns=["TYPE_RELATION"], errors="ignore")
            if not subset.empty:
                results_by_type[groupe].append((site_label, subset))

        # nettoyage CSV site
        try:
            if csv_path.exists():
                csv_path.unlink()
        except Exception:
            pass

    # G√©n√©ration Excel unique
    output_path = DATA_DIR / "doublons_all_sites.xlsx"
    try:
        if output_path.exists():
            output_path.unlink()
    except Exception:
        pass

    try:
        with st.spinner("üßæ G√©n√©ration du fichier Excel unique..."):
            with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
                for groupe in ["VRAI_DOUBLON", "DOUBLON_0_NUIT", "CONTIGUS", "CHEVAUCHEMENT"]:
                    keep_ghm = (groupe == "CONTIGUS")

                    if not results_by_type[groupe]:
                        # Aucun site n'a de r√©sultat : onglet placeholder
                        if keep_ghm:
                            df_to_write = pd.DataFrame([{
                                "IPP": "N/A", "DATE_NAISSANCE": "N/A",
                                "NDA": "N/A", "UMA": "N/A", "GHM": "N/A",
                                "DATE_ENTREE": "N/A", "DATE_SORTIE": "N/A",
                                "SITE": "N/A", "COMMENTAIRE": ""
                            }])
                        else:
                            df_to_write = pd.DataFrame([{
                                "IPP": "N/A", "DATE_NAISSANCE": "N/A",
                                "NDA": "N/A", "UMA": "N/A",
                                "DATE_ENTREE": "N/A", "DATE_SORTIE": "N/A",
                                "SITE": "N/A", "COMMENTAIRE": ""
                            }])
                    else:
                        blocks = []
                        for site_label, subset in results_by_type[groupe]:
                            blocks.append(build_rows_instead_of_columns(subset, keep_ghm=keep_ghm, site_label=site_label))
                        df_to_write = pd.concat(blocks, ignore_index=True)

                    df_to_write.to_excel(writer, sheet_name=groupe, index=False)

                    ws = writer.sheets[groupe]
                    for i, col in enumerate(df_to_write.columns):
                        max_len = max(df_to_write[col].astype(str).map(len).max(), len(col)) + 2
                        ws.column_dimensions[ws.cell(row=1, column=i + 1).column_letter].width = max_len

    except Exception as e:
        st.error(f"Erreur √©criture Excel : {e}")
        st.stop()

    st.success("‚úÖ Analyse termin√©e (3 sites) !")

    try:
        excel_bytes = output_path.read_bytes()
    except Exception as e:
        st.error(f"Impossible de lire le fichier Excel g√©n√©r√© : {e}")
        st.stop()

    st.download_button(
        label="üì• T√©l√©charger le fichier Excel (3 sites)",
        data=excel_bytes,
        file_name=f"doublons_CCH_HTD_BRC_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

    # nettoyage excel temporaire
    try:
        if output_path.exists():
            output_path.unlink()
    except Exception:
        pass


if __name__ == "__main__":
    main()
