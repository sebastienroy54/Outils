def main():
    st.title("D√©tection des Doublons, Contigus et Chevauchements")
    st.markdown(
        "Cet outil ex√©cute la requ√™te SQL, analyse les s√©jours "
        "et g√©n√®re un fichier Excel structur√©."
    )

    with st.form("connexion"):
        st.subheader("Connexion Oracle")
        user = st.text_input("üë§ Nom d'utilisateur Oracle :", value="")
        password = st.text_input("üîë Mot de passe :", type="password")
        submitted = st.form_submit_button("üöÄ Lancer le traitement")

    if not submitted:
        return

    if not user or not password:
        st.error("Veuillez renseigner vos identifiants Oracle.")
        st.stop()

    # ‚ö†Ô∏è Remplace par EZCONNECT si possible
    tns_alias = "o-simpa-b1.cch.aphp.fr"

    # 1) Test de connexion (ta fonction corrig√©e)
    with st.spinner("üîê Test de connexion Oracle..."):
        if not test_oracle_connection(user, password, tns_alias):
            # test_oracle_connection affiche d√©j√† l'erreur si tu as appliqu√© ma version
            st.stop()

    # 2) Ex√©cution SQL (g√©n√®re le CSV)
    with st.spinner("üß† Ex√©cution de la requ√™te SQL, veuillez patienter..."):
        result = subprocess.run(
            ["sqlplus", "-S", f"{user}/{password}@{tns_alias}", f"@{MAIN_SQL}"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        if result.returncode != 0:
            st.error("Erreur lors de l'ex√©cution SQL*Plus")
            st.code(result.stderr or result.stdout)
            st.stop()

    st.success("‚úÖ Requ√™te SQL termin√©e et fichier CSV g√©n√©r√©.")
    st.divider()

    if not CSV_PATH.exists():
        st.error(
            f"Le fichier {CSV_PATH.name} n'existe pas apr√®s ex√©cution SQL. "
            "V√©rifie le spool dans contigus.sql et le chemin de sortie."
        )
        st.stop()

    # 3) Lecture / nettoyage donn√©es
    with st.spinner("üîé Analyse des s√©jours en cours..."):
        try:
            df = pd.read_csv(
                CSV_PATH,
                sep=";",
                skip_blank_lines=True,
                dtype=str,
                encoding="latin-1"
            )
        except Exception as e:
            st.error(f"Erreur lecture CSV : {e}")
            st.stop()

        try:
            df = df.dropna(how="all")
            df.columns = [col.strip().upper() for col in df.columns]

            expected_cols = [
                "IPP", "DATE_NAISSANCE", "NDA", "UMA",
                "GHM", "DATE_ENTREE", "DATE_SORTIE"
            ]
            if df.shape[1] >= len(expected_cols):
                df.columns = expected_cols + [
                    f"COL_EXTRA_{i}"
                    for i in range(df.shape[1] - len(expected_cols))
                ]

            df["IPP"] = df["IPP"].astype(str).str.strip()
            df["UMA"] = df["UMA"].fillna("").astype(str).str.strip()
            df["NDA"] = df["NDA"].astype(str).str.strip()
            df["GHM"] = df["GHM"].fillna("").astype(str).str.strip()

            df["DATE_ENTREE"] = pd.to_datetime(df["DATE_ENTREE"], dayfirst=True, errors="coerce")
            df["DATE_SORTIE"] = pd.to_datetime(df["DATE_SORTIE"], dayfirst=True, errors="coerce")

            df = df[~df["UMA"].isin(["540", "543"])]
            df = df.dropna(subset=["DATE_ENTREE", "DATE_SORTIE", "UMA", "IPP"])
            df = df.sort_values(["IPP", "DATE_ENTREE", "DATE_SORTIE"]).reset_index(drop=True)

        except Exception as e:
            st.error(f"Erreur nettoyage donn√©es : {e}")
            st.stop()

        # 4) D√©tection relations
        rows = []
        try:
            for ipp, group in df.groupby("IPP", sort=False):
                g = group.reset_index(drop=True)
                n = len(g)

                for i in range(n):
                    nda1, uma1, ghm1, de1, ds1 = g.loc[
                        i, ["NDA", "UMA", "GHM", "DATE_ENTREE", "DATE_SORTIE"]
                    ]

                    j = i + 1
                    while j < n:
                        de2 = g.loc[j, "DATE_ENTREE"]
                        if pd.isna(de2):
                            j += 1
                            continue
                        if de2 > ds1 and de2 != ds1:
                            break

                        nda2, uma2, ghm2, ds2 = g.loc[
                            j, ["NDA", "UMA", "GHM", "DATE_SORTIE"]
                        ]

                        if nda1 == nda2:
                            j += 1
                            continue

                        rel = None

                        if (uma1 == uma2) and (de1 == de2) and (ds1 == ds2):
                            rel = "VRAI_DOUBLON"
                        elif (uma1 != uma2) and (de1 == de2) and (ds1 == ds2):
                            rel = "DOUBLON_0_NUIT"
                        else:
                            root1 = str(ghm1).strip()
                            root2 = str(ghm2).strip()
                            valid_ghm = (
                                root1 not in ("", "NAN", "None", "NONE")
                                and root2 not in ("", "NAN", "None", "NONE")
                                and len(root1) >= 5
                                and len(root2) >= 5
                            )

                            if (
                                uma1 != uma2
                                and ((ds1 == de2) or (ds2 == de1))
                                and valid_ghm
                                and root1[:5] == root2[:5]
                            ):
                                rel = "CONTIGUS"
                            elif (uma1 != uma2) and (de1 < ds2) and (ds1 > de2):
                                rel = "CHEVAUCHEMENT"

                        if rel:
                            rows.append({
                                "IPP": ipp,
                                "DATE_NAISSANCE": g.loc[i, "DATE_NAISSANCE"],
                                "NDA1": nda1,
                                "UMA1": uma1,
                                "GHM1": ghm1,
                                "DATEE1": de1,
                                "DATES1": ds1,
                                "NDA2": nda2,
                                "UMA2": uma2,
                                "GHM2": ghm2,
                                "DATEE2": de2,
                                "DATES2": ds2,
                                "TYPE_RELATION": rel
                            })

                        j += 1

        except Exception as e:
            st.error(f"Erreur d√©tection relations : {e}")
            st.stop()

        # 5) Ecriture Excel + t√©l√©chargement (puis nettoyage fichiers data/)
        try:
            res = pd.DataFrame(rows).drop_duplicates()

            with pd.ExcelWriter(OUTPUT_PATH, engine="openpyxl") as writer:
                for groupe in ["VRAI_DOUBLON", "DOUBLON_0_NUIT", "CONTIGUS", "CHEVAUCHEMENT"]:
                    subset = res[res["TYPE_RELATION"] == groupe].copy()

                    if groupe == "CONTIGUS":
                        cols = [
                            "IPP", "DATE_NAISSANCE", "NDA1", "UMA1",
                            "GHM1", "DATEE1", "DATES1",
                            "NDA2", "UMA2", "GHM2", "DATEE2", "DATES2"
                        ]
                        subset = subset.drop(columns=["TYPE_RELATION"], errors="ignore")
                    else:
                        cols = [
                            "IPP", "DATE_NAISSANCE", "NDA1", "UMA1",
                            "DATEE1", "DATES1", "NDA2", "UMA2",
                            "DATEE2", "DATES2"
                        ]
                        subset = subset.drop(columns=["GHM1", "GHM2", "TYPE_RELATION"], errors="ignore")

                    if subset.empty:
                        df_to_write = pd.DataFrame([{col: "N/A" for col in cols}])
                    else:
                        df_to_write = subset.copy()
                        for c in ["DATEE1", "DATES1", "DATEE2", "DATES2"]:
                            df_to_write[c] = pd.to_datetime(df_to_write[c], errors="coerce").dt.strftime("%d/%m/%Y")

                    df_to_write.to_excel(writer, sheet_name=groupe, index=False)

                    worksheet = writer.sheets[groupe]
                    for i, col in enumerate(df_to_write.columns):
                        max_len = max(df_to_write[col].astype(str).map(len).max(), len(col)) + 2
                        worksheet.column_dimensions[
                            worksheet.cell(row=1, column=i + 1).column_letter
                        ].width = max_len

        except Exception as e:
            st.error(f"Erreur √©criture Excel : {e}")
            st.stop()

    st.success("‚úÖ Analyse termin√©e avec succ√®s !")

    # Charger le fichier en m√©moire pour pouvoir le supprimer ensuite
    try:
        excel_bytes = OUTPUT_PATH.read_bytes()
    except Exception as e:
        st.error(f"Impossible de lire le fichier Excel g√©n√©r√© : {e}")
        st.stop()

    st.download_button(
        label="üì• T√©l√©charger le fichier Excel",
        data=excel_bytes,
        file_name=f"doublons_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

    # Nettoyage: on √©vite d'encombrer /data
    # (si tu pr√©f√®res garder l'Excel tant que non t√©l√©charg√©, on peut le faire autrement)
    for p in (CSV_PATH, OUTPUT_PATH):
        try:
            if p.exists():
                p.unlink()
        except Exception:
            pass
