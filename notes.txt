import streamlit as st
import pandas as pd
import subprocess
from io import BytesIO
from datetime import datetime
from pathlib import Path
import tempfile

BASE_DIR = Path(__file__).resolve().parents[1]
SQL_DIR = BASE_DIR / "SQL"
DATA_DIR = BASE_DIR / "data"

TEST_SQL = SQL_DIR / "test_connexion.sql"
MAIN_SQL = SQL_DIR / "contigus.sql"
CSV_PATH = DATA_DIR / "resultat.csv"
OUTPUT_PATH = DATA_DIR / "doublons.xlsx"

def test_oracle_connection(user, password, tns_alias):
    cmd = ["sqlplus", "-S", f"{user}/{password}@{tns_alias}", f"@{TEST_SQL}"]

    try:
        result = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=10
        )

        if result.returncode != 0:
            st.error("Connexion Oracle √©chou√©e")
            st.code(result.stderr or result.stdout)
            return False

        return True

    except subprocess.TimeoutExpired:
        st.error("Connexion Oracle: timeout")
        return False
    except Exception as e:
        st.error(f"Connexion Oracle: erreur inattendue: {e}")
        return False

def main():
    st.title("D√©tection des Doublons, Contigus et Chevauchements")
    st.markdown(
        "Cet outil ex√©cute la requ√™te SQL, analyse les s√©jours "
        "et g√©n√®re un fichier Excel structur√©."
    )

    with st.form("connexion"):
        st.subheader("Connexion Oracle")
        user = st.text_input("üë§ Nom d'utilisateur Oracle :", value="")
        password = st.text_input("üîë Mot de passe :", type="password")
        submitted = st.form_submit_button("üöÄ Lancer le traitement")

    if not submitted:
        return

    if not user or not password:
        st.error("Veuillez renseigner vos identifiants Oracle.")
        st.stop()

    tns_alias = "//o-simpa-b1.cch.aphp.fr:10805/SIP1CCH"

    # Nettoyage pr√©ventif : si un ancien CSV tra√Æne, on l‚Äôenl√®ve
    try:
        if CSV_PATH.exists():
            CSV_PATH.unlink()
    except Exception:
        pass

    # 1) Test connexion (doit stopper si KO)
    with st.spinner("üîê Test de connexion Oracle..."):
        ok = test_oracle_connection(user, password, tns_alias)
        if not ok:
            # Si ta fonction n‚Äôaffiche pas d√©j√† les d√©tails :
            st.error("Connexion Oracle √©chou√©e.")
            st.stop()

    # 2) Ex√©cution SQL (g√©n√®re le CSV)
    with st.spinner("üß† Ex√©cution de la requ√™te SQL, veuillez patienter..."):
        result = subprocess.run(
            ["sqlplus", "-S", f"{user}/{password}@{tns_alias}", f"@{MAIN_SQL}"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        if result.returncode != 0:
            st.error("Erreur lors de l'ex√©cution SQL*Plus")
            #st.code(result.stderr or result.stdout)
            st.stop()

    # 3) V√©rifier que le CSV a bien √©t√© produit
    if not CSV_PATH.exists():
        st.error(
            f"Le fichier {CSV_PATH.name} n'existe pas apr√®s ex√©cution SQL. "
            "V√©rifie le spool dans contigus.sql et le chemin de sortie."
        )
        st.code(result.stderr or result.stdout)
        st.stop()

    st.success("‚úÖ Requ√™te SQL termin√©e et fichier CSV g√©n√©r√©.")
    st.divider()

    # 4) Lecture / pr√©paration des donn√©es
    with st.spinner("üîé Analyse des s√©jours en cours..."):
        try:
            df = pd.read_csv(
                CSV_PATH,
                sep=";",
                skip_blank_lines=True,
                dtype=str,
                encoding="latin-1"
            )
        except Exception as e:
            st.error(f"Erreur lecture CSV : {e}")
            st.stop()

        try:
            df = df.dropna(how="all")
            df.columns = [col.strip().upper() for col in df.columns]

            expected_cols = [
                "IPP", "DATE_NAISSANCE", "NDA", "UMA",
                "GHM", "DATE_ENTREE", "DATE_SORTIE"
            ]
            if df.shape[1] >= len(expected_cols):
                df.columns = expected_cols + [
                    f"COL_EXTRA_{i}"
                    for i in range(df.shape[1] - len(expected_cols))
                ]

            df["IPP"] = df["IPP"].astype(str).str.strip()
            df["UMA"] = df["UMA"].fillna("").astype(str).str.strip()
            df["NDA"] = df["NDA"].astype(str).str.strip()
            df["GHM"] = df["GHM"].fillna("").astype(str).str.strip()

            df["DATE_ENTREE"] = pd.to_datetime(df["DATE_ENTREE"], dayfirst=True, errors="coerce")
            df["DATE_SORTIE"] = pd.to_datetime(df["DATE_SORTIE"], dayfirst=True, errors="coerce")

            df = df[~df["UMA"].isin(["540", "543"])]
            df = df.dropna(subset=["DATE_ENTREE", "DATE_SORTIE", "UMA", "IPP"])
            df = df.sort_values(["IPP", "DATE_ENTREE", "DATE_SORTIE"]).reset_index(drop=True)

        except Exception as e:
            st.error(f"Erreur nettoyage donn√©es : {e}")
            st.stop()

    # 5) D√©tection des relations
    rows = []
    try:
        for ipp, group in df.groupby("IPP", sort=False):
            g = group.reset_index(drop=True)
            n = len(g)

            for i in range(n):
                nda1, uma1, ghm1, de1, ds1 = g.loc[
                    i, ["NDA", "UMA", "GHM", "DATE_ENTREE", "DATE_SORTIE"]
                ]

                j = i + 1
                while j < n:
                    de2 = g.loc[j, "DATE_ENTREE"]
                    if pd.isna(de2):
                        j += 1
                        continue
                    if de2 > ds1 and de2 != ds1:
                        break

                    nda2, uma2, ghm2, ds2 = g.loc[
                        j, ["NDA", "UMA", "GHM", "DATE_SORTIE"]
                    ]

                    if nda1 == nda2:
                        j += 1
                        continue

                    rel = None
                    if (uma1 == uma2) and (de1 == de2) and (ds1 == ds2):
                        rel = "VRAI_DOUBLON"
                    elif (uma1 != uma2) and (de1 == de2) and (ds1 == ds2):
                        rel = "DOUBLON_0_NUIT"
                    else:
                        root1 = str(ghm1).strip()
                        root2 = str(ghm2).strip()
                        valid_ghm = (
                            root1 not in ("", "NAN", "None", "NONE")
                            and root2 not in ("", "NAN", "None", "NONE")
                            and len(root1) >= 5
                            and len(root2) >= 5
                        )

                        if (
                            uma1 != uma2
                            and ((ds1 == de2) or (ds2 == de1))
                            and valid_ghm
                            and root1[:5] == root2[:5]
                        ):
                            rel = "CONTIGUS"
                        elif (uma1 != uma2) and (de1 < ds2) and (ds1 > de2):
                            rel = "CHEVAUCHEMENT"

                    if rel:
                        rows.append({
                            "IPP": ipp,
                            "DATE_NAISSANCE": g.loc[i, "DATE_NAISSANCE"],

                            "NDA1": nda1, "UMA1": uma1, "GHM1": ghm1,
                            "DATEE1": de1, "DATES1": ds1,

                            "NDA2": nda2, "UMA2": uma2, "GHM2": ghm2,
                            "DATEE2": de2, "DATES2": ds2,

                            "TYPE_RELATION": rel
                        })

                    j += 1

    except Exception as e:
        st.error(f"Erreur d√©tection relations : {e}")
        st.stop()

    # 6) Construire l‚ÄôExcel ‚Äúen lignes‚Äù
    try:
        res = pd.DataFrame(rows).drop_duplicates().reset_index(drop=True)
        res["PAIR_ID"] = res.index + 1

        def build_rows_instead_of_columns(df_rel, keep_ghm: bool):
            left_cols = {
                "IPP": "IPP",
                "DATE_NAISSANCE": "DATE_NAISSANCE",
                "NDA": "NDA1",
                "UMA": "UMA1",
                "DATE_ENTREE": "DATEE1",
                "DATE_SORTIE": "DATES1",
                "PAIR_ID": "PAIR_ID",
            }
            if keep_ghm:
                left_cols["GHM"] = "GHM1"

            df1 = df_rel[list(left_cols.values())].copy()
            df1.columns = list(left_cols.keys())
            df1["LIGNE"] = 1

            right_cols = {
                "IPP": "IPP",
                "DATE_NAISSANCE": "DATE_NAISSANCE",
                "NDA": "NDA2",
                "UMA": "UMA2",
                "DATE_ENTREE": "DATEE2",
                "DATE_SORTIE": "DATES2",
                "PAIR_ID": "PAIR_ID",
            }
            if keep_ghm:
                right_cols["GHM"] = "GHM2"

            df2 = df_rel[list(right_cols.values())].copy()
            df2.columns = list(right_cols.keys())
            df2["LIGNE"] = 2

            out = pd.concat([df1, df2], ignore_index=True)

            for c in ["DATE_ENTREE", "DATE_SORTIE"]:
                out[c] = pd.to_datetime(out[c], errors="coerce").dt.strftime("%d/%m/%Y")

            out["COMMENTAIRE"] = ""

            if keep_ghm:
                out = out[["IPP", "DATE_NAISSANCE", "NDA", "UMA", "GHM",
                           "DATE_ENTREE", "DATE_SORTIE", "COMMENTAIRE"]]
            else:
                out = out[["IPP", "DATE_NAISSANCE", "NDA", "UMA",
                           "DATE_ENTREE", "DATE_SORTIE", "COMMENTAIRE"]]

            out = out.sort_values(["IPP"]).reset_index(drop=True)
            return out

        with pd.ExcelWriter(OUTPUT_PATH, engine="openpyxl") as writer:
            for groupe in ["VRAI_DOUBLON", "DOUBLON_0_NUIT", "CONTIGUS", "CHEVAUCHEMENT"]:
                subset = res[res["TYPE_RELATION"] == groupe].copy()
                subset = subset.drop(columns=["TYPE_RELATION"], errors="ignore")

                keep_ghm = (groupe == "CONTIGUS")

                if subset.empty:
                    if keep_ghm:
                        df_to_write = pd.DataFrame([{
                            "IPP": "N/A", "DATE_NAISSANCE": "N/A",
                            "NDA": "N/A", "UMA": "N/A", "GHM": "N/A",
                            "DATE_ENTREE": "N/A", "DATE_SORTIE": "N/A", "COMMENTAIRE": ""
                        }])
                    else:
                        df_to_write = pd.DataFrame([{
                            "IPP": "N/A", "DATE_NAISSANCE": "N/A",
                            "NDA": "N/A", "UMA": "N/A",
                            "DATE_ENTREE": "N/A", "DATE_SORTIE": "N/A", "COMMENTAIRE": ""
                        }])
                else:
                    df_to_write = build_rows_instead_of_columns(subset, keep_ghm=keep_ghm)

                df_to_write.to_excel(writer, sheet_name=groupe, index=False)

                ws = writer.sheets[groupe]
                for i, col in enumerate(df_to_write.columns):
                    max_len = max(df_to_write[col].astype(str).map(len).max(), len(col)) + 2
                    ws.column_dimensions[ws.cell(row=1, column=i + 1).column_letter].width = max_len

    except Exception as e:
        st.error(f"Erreur √©criture Excel : {e}")
        st.stop()

    st.success("‚úÖ Analyse termin√©e avec succ√®s !")

    # 7) Download + nettoyage
    try:
        excel_bytes = OUTPUT_PATH.read_bytes()
    except Exception as e:
        st.error(f"Impossible de lire le fichier Excel g√©n√©r√© : {e}")
        st.stop()

    st.download_button(
        label="üì• T√©l√©charger le fichier Excel",
        data=excel_bytes,
        file_name=f"doublons_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

    # Nettoyage: on √©vite d‚Äôencombrer /data
    for p in (CSV_PATH, OUTPUT_PATH):
        try:
            if p.exists():
                p.unlink()
        except Exception:
            pass
